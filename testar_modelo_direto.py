#!/usr/bin/env python3
"""
Script simples para testar o modelo diretamente (sem API)
Uso: python3 testar_modelo_direto.py "sua pergunta aqui"
"""

import sys
import os
import time
from pathlib import Path

# Adiciona o diret√≥rio raiz ao path
sys.path.insert(0, str(Path(__file__).parent))

from src.models.base_model import CodeLlamaBaseModel
from src.models.api_model import APIModel
from src.utils.config import get_config
from src.utils.logging import get_logger

logger = get_logger("test_modelo_direto")


def main():
    """Testa o modelo diretamente"""
    # Pega a pergunta dos argumentos ou usa padr√£o
    if len(sys.argv) > 1:
        query = " ".join(sys.argv[1:])
    else:
        query = "Ol√°! Como voc√™ est√°?"
    
    print(f"ü§ñ Testando modelo diretamente...")
    print(f"üìù Pergunta: {query}")
    print(f"{'='*60}\n")
    
    try:
        # Inicializa modelo (local ou API)
        print("‚è≥ Carregando modelo...")
        start_init = time.time()
        
        config = get_config()
        model_config = config.model
        model_mode = model_config.mode or os.getenv('MODEL_MODE', 'local')
        
        if model_mode == "api":
            provider = model_config.provider or os.getenv('MODEL_PROVIDER', 'groq')
            print(f"üåê Usando API: {provider}")
            model = APIModel(provider=provider)
        else:
            print("üíª Usando modelo local")
            model = CodeLlamaBaseModel()
        
        init_time = time.time() - start_init
        print(f"‚úÖ Modelo inicializado em {init_time:.2f}s\n")
        
        # Gera resposta com streaming
        print("‚è≥ Gerando resposta (streaming)...\n")
        print(f"{'='*60}")
        print("ü§ñ Resposta (streaming):\n")
        
        start_gen = time.time()
        response_parts = []
        
        # Itera sobre os tokens conforme s√£o gerados
        # Sem limite de tokens - gera resposta completa
        try:
            for token in model.generate(
                query,
                max_length=8192,  # Sem limite - resposta completa
                temperature=0.7,
                stream=True
            ):
                # Imprime token imediatamente (sem quebra de linha)
                print(token, end='', flush=True)
                response_parts.append(token)
        except KeyboardInterrupt:
            print("\n\n‚ö†Ô∏è Interrompido pelo usu√°rio")
        except Exception as e:
            print(f"\n\n‚ùå Erro durante streaming: {e}")
            import traceback
            traceback.print_exc()
        
        gen_time = time.time() - start_gen
        full_response = ''.join(response_parts)
        
        # Mostra resultado
        print(f"\n\n{'='*60}")
        print(f"‚úÖ Resposta gerada em {gen_time:.2f}s")
        print(f"{'='*60}")
        print(f"üìä Estat√≠sticas:")
        print(f"   ‚Ä¢ Tempo de inicializa√ß√£o: {init_time:.2f}s")
        print(f"   ‚Ä¢ Tempo de gera√ß√£o: {gen_time:.2f}s")
        print(f"   ‚Ä¢ Tempo total: {init_time + gen_time:.2f}s")
        print(f"   ‚Ä¢ Tamanho da resposta: {len(full_response)} caracteres")
        print(f"   ‚Ä¢ Tokens gerados: {len(response_parts)}")
        
    except Exception as e:
        print(f"\n‚ùå Erro: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

