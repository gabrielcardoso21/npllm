# üß™ Teste Local com Modelo Menor

**Objetivo**: Iterar rapidamente localmente antes de testar no Contabo.

## üìã Configura√ß√£o

### Modelo Local (Testes R√°pidos)
- **Modelo**: `TinyLlama/TinyLlama-1.1B-Chat-v1.0`
- **Tamanho**: ~600MB
- **Config**: `config/default.yaml`

### Modelo Produ√ß√£o (Contabo)
- **Modelo**: `bigcode/starcoder2-3b`
- **Tamanho**: ~6GB
- **Config**: `config/production.yaml`

## üöÄ Como Usar

### Teste Local (Modelo Menor)
```bash
# Ativar ambiente virtual
source .venv/bin/activate

# Testar modelo diretamente
python3 -c "from src.models.base_model import CodeLlamaBaseModel; m = CodeLlamaBaseModel(); print(m.generate('Ol√°!', max_length=50))"

# Testar sistema completo
python3 -c "from src.main import initialize_system; s = initialize_system(); print(s.process_query('Ol√°!'))"

# Testar API local
python3 -m src.api.server --host 0.0.0.0 --port 8000
```

### Produ√ß√£o (Modelo Maior)
No Contabo, usar `config/production.yaml` ou definir vari√°vel de ambiente:
```bash
export MODEL_BASE_MODEL="bigcode/starcoder2-3b"
```

## ‚ö° Vantagens do Modelo Menor

- ‚úÖ **Carregamento r√°pido**: ~5-10 segundos vs ~30-60 segundos
- ‚úÖ **Menos RAM**: ~1GB vs ~6GB
- ‚úÖ **Itera√ß√£o r√°pida**: Testes em segundos
- ‚úÖ **Mesma arquitetura**: C√≥digo funciona igual

## ‚ö†Ô∏è Limita√ß√µes

- ‚ùå **Qualidade menor**: Respostas menos precisas
- ‚ùå **Contexto menor**: Menos tokens de contexto
- ‚ö†Ô∏è **Apenas para testes**: N√£o usar em produ√ß√£o

