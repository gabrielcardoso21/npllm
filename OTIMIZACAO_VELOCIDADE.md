# ‚ö° Guia de Otimiza√ß√£o de Velocidade

## ‚úÖ Corre√ß√µes Aplicadas

1. **Erro de logging corrigido**: Formato simplificado (sem JSON complexo)
2. **max_length reduzido**: 128 tokens (antes: 256) para itera√ß√£o mais r√°pida
3. **Otimiza√ß√£o autom√°tica**: TinyLlama limita automaticamente a 128 tokens
4. **max_new_tokens**: C√°lculo mais preciso e eficiente

## üöÄ Resultado Esperado

- **Antes**: ~76 segundos para 111 tokens
- **Agora**: ~10-20 segundos para 128 tokens (estimado)
- **Melhoria**: ~4-7x mais r√°pido

## üìä Como Testar

```bash
python3 testar_modelo_direto.py "O que √© Odoo?"
```

## üîß Otimiza√ß√µes Adicionais (se ainda estiver lento)

### 1. Reduzir ainda mais o max_length

Edite `testar_modelo_direto.py` linha 48:
```python
max_length=64,  # Ainda mais r√°pido (respostas curtas)
```

### 2. Usar modelo ainda menor (se dispon√≠vel)

Modelos muito pequenos para testes ultra-r√°pidos:
- `TinyLlama/TinyLlama-1.1B-Chat-v1.0` (atual) - ~600MB
- `microsoft/phi-1_5` - ~1.3GB (mais r√°pido que TinyLlama)
- `Qwen/Qwen2-0.5B` - ~1GB (muito r√°pido)

Para trocar, edite `config/default.yaml`:
```yaml
model:
  base_model: "microsoft/phi-1_5"  # Mais r√°pido
```

### 3. Reduzir temperatura (mais determin√≠stico)

```python
temperature=0.3,  # Menos aleat√≥rio = mais r√°pido
```

### 4. Desabilitar sampling completamente

```python
temperature=0.0,  # Greedy decoding (mais r√°pido)
do_sample=False,
```

### 5. Usar CPU com otimiza√ß√µes

Se tiver CPU moderno, pode usar:
- `torch.compile()` (PyTorch 2.0+)
- ONNX Runtime
- Intel Extension for PyTorch

## üìà Benchmarks Esperados

| Configura√ß√£o | Tempo (128 tokens) | Qualidade |
|-------------|-------------------|-----------|
| TinyLlama + 128 tokens | ~10-20s | Boa |
| TinyLlama + 64 tokens | ~5-10s | Razo√°vel |
| phi-1_5 + 128 tokens | ~8-15s | Melhor |
| phi-1_5 + 64 tokens | ~4-8s | Boa |

## üéØ Para Produ√ß√£o

Quando estiver pronto para produ√ß√£o, use:
- `bigcode/starcoder2-3b` (melhor qualidade)
- `max_length=512` ou mais
- GPU se dispon√≠vel

## üí° Dicas

1. **Primeira execu√ß√£o**: Sempre mais lenta (carrega modelo)
2. **Execu√ß√µes seguintes**: Mais r√°pidas (modelo em mem√≥ria)
3. **Streaming**: Voc√™ v√™ tokens em tempo real (melhor UX)
4. **Cache**: Respostas id√™nticas s√£o instant√¢neas

## üîç Troubleshooting

**Ainda muito lento?**
- Verifique recursos: `free -h` e `nproc`
- Reduza `max_length` para 64 ou 32
- Use `temperature=0.0` para greedy decoding

**Respostas muito curtas?**
- Aumente `max_length` para 256 ou 512
- Mas lembre-se: mais tokens = mais tempo

**Erro de mem√≥ria?**
- Use modelo menor
- Reduza `max_length`
- Feche outros programas

