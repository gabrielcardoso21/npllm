# ğŸ”„ Streaming com Adapter LoRA

## ğŸ“‹ Problema Resolvido

**Problema original**: No streaming, tokens sÃ£o enviados um por um, mas o adapter LoRA precisa da resposta completa para revisar.

**SoluÃ§Ã£o implementada**: O adapter LoRA Ã© carregado no modelo base **ANTES** da geraÃ§Ã£o, entÃ£o os tokens jÃ¡ vÃªm "adaptados" durante o streaming.

## ğŸ—ï¸ Arquitetura

### Fluxo de Streaming com Adapter

```
1. Seleciona adapter baseado no contexto
2. Carrega adapter no modelo base (PEFT)
3. Gera tokens com streaming (jÃ¡ adaptados)
4. Envia tokens em tempo real
5. Finaliza com resposta completa
```

### Como Funciona

1. **SeleÃ§Ã£o de Adapter**: O sistema seleciona o adapter apropriado (ex: `python_adapter`, `odoo_adapter`)

2. **Carregamento**: O adapter Ã© carregado no modelo base usando `PeftModel.from_pretrained()`
   - Isso modifica os pesos do modelo para incluir os pesos do adapter
   - A geraÃ§Ã£o subsequente jÃ¡ usa o adapter

3. **GeraÃ§Ã£o com Streaming**: 
   - Tokens sÃ£o gerados um por um
   - Cada token jÃ¡ estÃ¡ "adaptado" porque o adapter estÃ¡ carregado
   - NÃ£o precisa esperar resposta completa

4. **Envio em Tempo Real**: Tokens sÃ£o enviados via SSE conforme sÃ£o gerados

## ğŸ“¡ Formato SSE

```
data: {"type": "start", "adapter": "loading"}
data: {"type": "adapter", "adapter": "python_adapter"}
data: {"type": "adapter_loaded", "adapter": "python_adapter"}
data: {"type": "token", "token": "def"}
data: {"type": "token", "token": " fibonacci"}
...
data: {"type": "done", "response": "...", "adapter_used": "python_adapter", "adapter_applied": true}
```

## âš¡ Vantagens

1. **Feedback Imediato**: Tokens aparecem em tempo real
2. **Adapter Aplicado**: Resposta jÃ¡ vem adaptada, nÃ£o precisa revisÃ£o posterior
3. **EficiÃªncia**: Adapter carregado uma vez, usado em todas as geraÃ§Ãµes
4. **Sem Perda de Qualidade**: Adapter Ã© aplicado durante geraÃ§Ã£o, nÃ£o depois

## ğŸ”§ ImplementaÃ§Ã£o TÃ©cnica

### Carregamento do Adapter

```python
# Em src/api/server.py
adapter = system.adapter_manager.get_adapter(adapter_name, prefer_stable=True)
if adapter:
    system.adapter_manager.load_adapter_for_generation(
        adapter_name, 
        system.base_model
    )
```

### GeraÃ§Ã£o com Adapter Carregado

```python
# Modelo base jÃ¡ tem adapter carregado via PEFT
generator = system.base_model.generate(query_text, max_length=512, stream=True)

# Tokens jÃ¡ vÃªm adaptados
for token in generator:
    yield token
```

## ğŸ“ Notas Importantes

1. **Carregamento Ãšnico**: Adapter Ã© carregado uma vez e reutilizado
2. **Cache**: Adapters carregados ficam em cache (`_loaded_adapters`)
3. **Fallback**: Se adapter nÃ£o existe, usa modelo base sem adapter
4. **Performance**: Carregar adapter tem custo, mas Ã© feito uma vez

## ğŸš€ Uso

```bash
curl -N -X POST "http://161.97.123.192:8000/query?stream=true" \
  -H "Content-Type: application/json" \
  -d '{"query": "Crie uma funÃ§Ã£o Python", "file_path": "main.py"}'
```

O sistema automaticamente:
1. Seleciona adapter baseado em `file_path` (ex: `.py` â†’ `python_adapter`)
2. Carrega adapter no modelo
3. Gera resposta com streaming (jÃ¡ adaptada)
4. Envia tokens em tempo real

