# Decis√£o NP-001: Plasticidade Sin√°ptica - Infraestrutura Base

**Status**: üü° Em Andamento  
**Data**: 2025-01-27  
**Decisor**: Gabriel Cardoso  
**Contexto**: Neuroplasticidade √© a base do projeto - sem ela, n√£o h√° processos psicol√≥gicos  
**Depend√™ncias**: Nenhuma (decis√£o fundamental)

---

## Contexto

A neuroplasticidade √© o mecanismo fundamental que permite ao c√©rebro adaptar-se e aprender. No contexto do npllm, a plasticidade sin√°ptica √© a infraestrutura base que permite:

1. **Aprendizado Cont√≠nuo**: Adapta√ß√£o sem esquecimento catastr√≥fico
2. **Mem√≥ria Persistente**: Consolida√ß√£o de conhecimento
3. **Adapta√ß√£o Contextual**: Ajuste baseado em experi√™ncia
4. **Evolu√ß√£o do Sistema**: Melhoria cont√≠nua ao longo do tempo

**Sem neuroplasticidade, o sistema n√£o pode aprender verdadeiramente** - apenas usar conhecimento pr√©-treinado.

---

## Pesquisa Profunda: Estado da Arte em Plasticidade Sin√°ptica para LLMs

### 1. Plasticidade Sin√°ptica Biol√≥gica

#### Fundamentos Neurocient√≠ficos

**LTP (Long-Term Potentiation)**:
- Fortalecimento de sinapses ap√≥s estimula√ß√£o repetida
- Base para aprendizado e mem√≥ria
- Persiste por horas a anos
- **Paper Cl√°ssico**: Bliss & Lomo (1973) - "Long-lasting potentiation of synaptic transmission"

**LTD (Long-Term Depression)**:
- Enfraquecimento de sinapses n√£o utilizadas
- Elimina√ß√£o de conex√µes desnecess√°rias
- Balanceamento com LTP
- **Mecanismo**: Timing-dependent, atividade-dependente

**Regra de Hebb (1949)**:
- "Neur√¥nios que disparam juntos, conectam-se juntos"
- Base para aprendizado associativo
- Fundamenta muitos algoritmos modernos

**STDP (Spike-Timing Dependent Plasticity)**:
- Plasticidade baseada em timing preciso
- Se pr√©-sin√°ptico dispara antes do p√≥s: LTP
- Se p√≥s-sin√°ptico dispara antes do pr√©: LTD
- **Relev√¢ncia**: Processamento temporal

---

### 2. Plasticidade Diferenci√°vel (Differentiable Plasticity)

#### Paper Fundamental

**"Differentiable plasticity: training plastic neural networks with backpropagation"**  
**Autores**: Miconi, T., Clune, J., & Stanley, K. O. (2018)  
**ArXiv**: [1804.02464](https://arxiv.org/abs/1804.02464)

**Conceito**:
- Permite que redes neurais aprendam a modificar suas pr√≥prias conex√µes
- Cada conex√£o tem: **peso base** + **peso pl√°stico**
- Peso pl√°stico adapta-se baseado em atividade
- Trein√°vel via backpropagation

**Mecanismo**:
```
Peso Total = Peso Base + (Peso Pl√°stico √ó Atividade)
```

**Caracter√≠sticas**:
- ‚úÖ Trein√°vel via backpropagation
- ‚úÖ Permite aprendizado cont√≠nuo
- ‚úÖ Redes adaptam-se ap√≥s treinamento inicial
- ‚úÖ Baseado em neuroci√™ncia real

**Aplica√ß√µes Demonstradas**:
- Aprendizado cont√≠nuo sem esquecimento catastr√≥fico
- Adapta√ß√£o r√°pida a novas tarefas
- Mem√≥ria de trabalho em redes neurais
- Transfer learning adaptativo

**Limita√ß√µes Identificadas**:
- ‚ö†Ô∏è Overhead computacional (par√¢metros adicionais)
- ‚ö†Ô∏è Testado principalmente em redes pequenas
- ‚ö†Ô∏è Hiperpar√¢metros precisam ajuste fino
- ‚ö†Ô∏è N√£o aplicado extensivamente em LLMs grandes

**Status Atual**:
- ‚úÖ C√≥digo dispon√≠vel (PyTorch, TensorFlow)
- ‚úÖ Implementa√ß√µes open-source
- ‚ö†Ô∏è N√£o amplamente usado em produ√ß√£o
- ‚ö†Ô∏è Ainda √°rea de pesquisa ativa

---

### 3. Backpropamine (Plasticidade Neuromodulada Diferenci√°vel)

#### Paper Fundamental

**"Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity"**  
**Autores**: Miconi, T., Rawal, A., Clune, J., & Stanley, K. O. (2020)  
**ArXiv**: [2002.10585](https://arxiv.org/abs/2002.10585)

**Conceito**:
- Estende plasticidade diferenci√°vel com **neuromodula√ß√£o**
- Neur√¥nios moduladores aprendem a controlar plasticidade de outros
- Similar a neurotransmissores (dopamina, serotonina) no c√©rebro
- Aprendizado end√≥geno de quando e onde modular

**Mecanismo**:
```
Sinal Modulador = f(Estado, Contexto)
Plasticidade = Plasticidade Base √ó Sinal Modulador
```

**Caracter√≠sticas**:
- ‚úÖ Neuromodula√ß√£o aprendida endogenamente
- ‚úÖ Controle din√¢mico da plasticidade
- ‚úÖ Melhor performance em RL e aprendizado supervisionado
- ‚úÖ Baseado em neuroci√™ncia real (dopamina, acetilcolina)

**Resultados Demonstrados**:
- Melhor performance em RL (at√© 2x)
- Aprendizado mais r√°pido
- Adapta√ß√£o contextual
- Sele√ß√£o de onde aprender

**Limita√ß√µes Identificadas**:
- ‚ö†Ô∏è Complexidade adicional
- ‚ö†Ô∏è Testado principalmente em modelos pequenos
- ‚ö†Ô∏è N√£o aplicado em LLMs de grande escala
- ‚ö†Ô∏è Requer mais pesquisa

**Status Atual**:
- ‚úÖ C√≥digo dispon√≠vel
- ‚úÖ Implementa√ß√µes em PyTorch/JAX
- ‚ö†Ô∏è Ainda experimental
- ‚ö†Ô∏è N√£o usado em produ√ß√£o LLMs

**Relev√¢ncia para npllm**:
- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **CR√çTICO** - √â exatamente o que o projeto precisa
- Alinhado com arquitetura biol√≥gica planejada
- Permite aprendizado cont√≠nuo real

---

### 4. Memory Aware Synapses (MAS)

#### Paper

**"Memory Aware Synapses: Learning what (not) to forget"**  
**Autores**: Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., & Tuytelaars, T. (2017)  
**ArXiv**: [1711.09601](https://arxiv.org/abs/1711.09601)

**Conceito**:
- Identifica automaticamente par√¢metros importantes
- Baseado em gradientes (n√£o requer dados de valida√ß√£o)
- Aprendizado n√£o-supervisionado de import√¢ncia

**Mecanismo**:
- Calcula import√¢ncia baseada em magnitude de gradientes
- Preserva par√¢metros importantes durante novo aprendizado
- Adaptativo e eficiente

**Vantagens**:
- ‚úÖ N√£o requer labels
- ‚úÖ Computacionalmente eficiente
- ‚úÖ Adaptativo
- ‚úÖ J√° implementado no projeto npllm

**Limita√ß√µes**:
- ‚ö†Ô∏è Pode ser conservador
- ‚ö†Ô∏è N√£o escala bem para modelos muito grandes
- ‚ö†Ô∏è N√£o √© plasticidade real (√© preserva√ß√£o)

**Status no Projeto**:
- ‚úÖ J√° implementado em `src/learning/continual_learning.py`
- ‚úÖ Funcional e testado
- ‚ö†Ô∏è MAS sozinho n√£o √© suficiente (precisa de Backpropamine)

---

### 5. Elastic Weight Consolidation (EWC)

#### Paper

**"Overcoming catastrophic forgetting in neural networks"**  
**Autores**: Kirkpatrick, J., et al. (2017)  
**PNAS**, 114(13), 3521-3526

**Conceito**:
- Calcula import√¢ncia de par√¢metros (Fisher Information Matrix)
- Adiciona penalidade para mudan√ßas em par√¢metros importantes
- Permite adapta√ß√£o de par√¢metros menos importantes

**Vantagens**:
- ‚úÖ Preserva conhecimento importante
- ‚úÖ Relativamente simples
- ‚úÖ Bem estudado

**Limita√ß√µes**:
- ‚ö†Ô∏è C√°lculo de Fisher Information √© caro
- ‚ö†Ô∏è N√£o escala bem para muitas tarefas
- ‚ö†Ô∏è Pode ser muito conservador
- ‚ö†Ô∏è N√£o √© plasticidade real (√© preserva√ß√£o)

**Status**:
- ‚úÖ Implementa√ß√µes dispon√≠veis
- ‚úÖ Usado em alguns sistemas
- ‚ö†Ô∏è N√£o √© solu√ß√£o completa

---

### 6. Aplica√ß√£o em LLMs: Estado Atual

#### O Que Existe Hoje

**1. Fine-tuning Tradicional**:
- ‚úÖ Amplamente usado
- ‚úÖ Funciona bem para adapta√ß√£o
- ‚ùå Causa esquecimento catastr√≥fico
- ‚ùå N√£o √© plasticidade real

**2. LoRA (Low-Rank Adaptation)**:
- ‚úÖ Eficiente computacionalmente
- ‚úÖ Preserva modelo base
- ‚ùå N√£o √© plasticidade real (√© adapter)
- ‚ùå N√£o existe no c√©rebro

**3. In-Context Learning**:
- ‚úÖ Aprendizado tempor√°rio no contexto
- ‚úÖ N√£o modifica pesos
- ‚ùå N√£o √© persistente
- ‚ùå N√£o √© plasticidade real

**4. RAG (Retrieval-Augmented Generation)**:
- ‚úÖ Mem√≥ria externa
- ‚úÖ N√£o modifica modelo
- ‚ùå N√£o consolida em par√¢metros
- ‚ùå N√£o √© plasticidade real

**5. Continual Learning T√©cnicas**:
- ‚úÖ EWC, MAS, Replay
- ‚úÖ Preservam conhecimento
- ‚ùå N√£o s√£o plasticidade real
- ‚ùå Focam em preserva√ß√£o, n√£o adapta√ß√£o

#### O Que N√ÉO Existe Hoje

**1. Plasticidade Real em LLMs Grandes**:
- ‚ùå Backpropamine n√£o foi aplicado em LLMs de 7B+ par√¢metros
- ‚ùå Differentiable Plasticity n√£o foi testado extensivamente
- ‚ùå Ainda √°rea de pesquisa

**2. Plasticidade Eficiente**:
- ‚ùå Overhead computacional √© alto
- ‚ùå N√£o escal√°vel para modelos muito grandes
- ‚ùå Precisa de otimiza√ß√µes

**3. Plasticidade Integrada**:
- ‚ùå N√£o h√° framework completo
- ‚ùå Integra√ß√£o com RAG √© experimental
- ‚ùå Consolida√ß√£o durante "sono" n√£o implementada

**4. Plasticidade em Produ√ß√£o**:
- ‚ùå Nenhum sistema de produ√ß√£o usa plasticidade real
- ‚ùå Ainda experimental
- ‚ùå Precisa mais pesquisa

---

### 7. Implementa√ß√µes Dispon√≠veis

#### C√≥digo Open Source

**1. Backpropamine**:
- ‚úÖ C√≥digo oficial dispon√≠vel
- ‚úÖ Implementa√ß√µes em PyTorch/JAX
- ‚ö†Ô∏è Focado em redes pequenas
- ‚ö†Ô∏è N√£o adaptado para Transformers/LLMs

**2. Differentiable Plasticity**:
- ‚úÖ Implementa√ß√µes em PyTorch
- ‚úÖ Exemplos dispon√≠veis
- ‚ö†Ô∏è N√£o para LLMs

**3. MAS/EWC**:
- ‚úÖ Implementa√ß√µes amplamente dispon√≠veis
- ‚úÖ J√° usado no projeto
- ‚ö†Ô∏è N√£o √© plasticidade real

#### Frameworks

**1. PyTorch**:
- ‚úÖ Suporte para gradientes customizados
- ‚úÖ Permite implementa√ß√£o de plasticidade
- ‚úÖ Flex√≠vel

**2. JAX**:
- ‚úÖ Facilita implementa√ß√£o de plasticidade
- ‚úÖ Performance boa
- ‚úÖ Gradientes autom√°ticos

**3. TensorFlow**:
- ‚úÖ Suporte para opera√ß√µes customizadas
- ‚úÖ Permite implementa√ß√£o
- ‚ö†Ô∏è Menos flex√≠vel que PyTorch

---

### 8. Desafios e Limita√ß√µes Atuais

#### Desafios T√©cnicos

1. **Escalabilidade**:
   - Plasticidade adiciona par√¢metros (pesos pl√°sticos)
   - Para LLM de 7B, adiciona ~7B par√¢metros pl√°sticos
   - Overhead de mem√≥ria e computa√ß√£o

2. **Estabilidade**:
   - Balancear adapta√ß√£o vs. estabilidade
   - Evitar instabilidade durante aprendizado
   - Hiperpar√¢metros cr√≠ticos

3. **Efici√™ncia Computacional**:
   - C√°lculo de plasticidade adiciona overhead
   - Precisa otimiza√ß√µes espec√≠ficas
   - Pode ser lento em modelos grandes

4. **Integra√ß√£o**:
   - Integrar com RAG
   - Integrar com consolida√ß√£o
   - Integrar com sistema dopamin√©rgico

#### Limita√ß√µes de Pesquisa

1. **Poucos Testes em LLMs Grandes**:
   - Maioria dos testes em redes pequenas
   - N√£o sabemos se escala para 7B+
   - Precisa valida√ß√£o

2. **Falta de Benchmarks**:
   - N√£o h√° benchmarks padronizados
   - Dif√≠cil comparar abordagens
   - M√©tricas n√£o padronizadas

3. **Falta de Frameworks**:
   - N√£o h√° framework completo
   - Cada implementa√ß√£o √© custom
   - Falta abstra√ß√µes

---

## Estado Atual: O Que Temos

### ‚úÖ Dispon√≠vel e Funcional

1. **Backpropamine**:
   - ‚úÖ C√≥digo dispon√≠vel
   - ‚úÖ Paper completo
   - ‚úÖ Implementa√ß√µes funcionais
   - ‚ö†Ô∏è N√£o testado em LLMs grandes

2. **Differentiable Plasticity**:
   - ‚úÖ C√≥digo dispon√≠vel
   - ‚úÖ Paper completo
   - ‚úÖ Implementa√ß√µes funcionais
   - ‚ö†Ô∏è N√£o testado em LLMs grandes

3. **MAS (Memory Aware Synapses)**:
   - ‚úÖ J√° implementado no projeto
   - ‚úÖ Funcional e testado
   - ‚úÖ Eficiente
   - ‚ö†Ô∏è N√£o √© plasticidade real (√© preserva√ß√£o)

4. **Frameworks de Suporte**:
   - ‚úÖ PyTorch (gradientes customizados)
   - ‚úÖ JAX (facilita implementa√ß√£o)
   - ‚úÖ TensorFlow (suporte b√°sico)

### ‚ö†Ô∏è Parcialmente Dispon√≠vel

1. **Continual Learning**:
   - ‚úÖ T√©cnicas dispon√≠veis (EWC, MAS, Replay)
   - ‚ö†Ô∏è N√£o s√£o plasticidade real
   - ‚ö†Ô∏è Focam em preserva√ß√£o

2. **Fine-tuning**:
   - ‚úÖ Amplamente usado
   - ‚ö†Ô∏è Causa esquecimento
   - ‚ö†Ô∏è N√£o √© aprendizado cont√≠nuo

---

## Estado Atual: O Que N√ÉO Temos

### ‚ùå N√£o Dispon√≠vel

1. **Plasticidade Real em LLMs Grandes**:
   - ‚ùå Backpropamine n√£o aplicado em 7B+
   - ‚ùå Differentiable Plasticity n√£o testado extensivamente
   - ‚ùå Ainda √°rea de pesquisa

2. **Plasticidade Eficiente**:
   - ‚ùå Overhead computacional alto
   - ‚ùå N√£o otimizado para Transformers
   - ‚ùå Precisa otimiza√ß√µes espec√≠ficas

3. **Framework Completo**:
   - ‚ùå N√£o h√° framework integrado
   - ‚ùå Cada implementa√ß√£o √© custom
   - ‚ùå Falta abstra√ß√µes

4. **Plasticidade em Produ√ß√£o**:
   - ‚ùå Nenhum sistema usa em produ√ß√£o
   - ‚ùå Ainda experimental
   - ‚ùå Precisa valida√ß√£o

5. **Integra√ß√£o Completa**:
   - ‚ùå Backpropamine + RAG n√£o integrado
   - ‚ùå Consolida√ß√£o durante "sono" n√£o implementada
   - ‚ùå Sistema dopamin√©rgico n√£o integrado

---

## Op√ß√µes Consideradas

### Op√ß√£o A: Backpropamine Puro

**Descri√ß√£o**:
- Implementar Backpropamine conforme paper original
- Aplicar diretamente no LLM base
- Plasticidade real em todas as camadas

**Vantagens**:
- ‚úÖ Plasticidade real (baseada em neuroci√™ncia)
- ‚úÖ Aprendizado cont√≠nuo verdadeiro
- ‚úÖ Alinhado com objetivos do projeto
- ‚úÖ C√≥digo dispon√≠vel

**Desvantagens**:
- ‚ö†Ô∏è Overhead alto (dobra par√¢metros)
- ‚ö†Ô∏è N√£o testado em LLMs grandes
- ‚ö†Ô∏è Pode ser inst√°vel
- ‚ö†Ô∏è Requer otimiza√ß√µes

**Implementa√ß√£o**:
- Usar c√≥digo oficial Backpropamine
- Adaptar para Transformers
- Integrar com LLM base

**Custo Computacional**:
- Mem√≥ria: ~2x (pesos base + pl√°sticos)
- Computa√ß√£o: ~1.5x (c√°lculo de plasticidade)

---

### Op√ß√£o B: Backpropamine Seletivo

**Descri√ß√£o**:
- Backpropamine apenas em camadas espec√≠ficas
- Exemplo: apenas em camadas de aten√ß√£o ou feed-forward
- Reduz overhead mantendo plasticidade

**Vantagens**:
- ‚úÖ Plasticidade real
- ‚úÖ Overhead reduzido
- ‚úÖ Mais eficiente
- ‚úÖ Pode ser mais est√°vel

**Desvantagens**:
- ‚ö†Ô∏è Plasticidade limitada
- ‚ö†Ô∏è Precisa decidir quais camadas
- ‚ö†Ô∏è Pode n√£o ser suficiente

**Implementa√ß√£o**:
- Identificar camadas cr√≠ticas
- Aplicar Backpropamine seletivamente
- Monitorar performance

**Custo Computacional**:
- Mem√≥ria: ~1.2-1.5x (depende de camadas)
- Computa√ß√£o: ~1.2x

---

### Op√ß√£o C: Backpropamine + MAS H√≠brido

**Descri√ß√£o**:
- Backpropamine para adapta√ß√£o r√°pida
- MAS para preserva√ß√£o de conhecimento importante
- Combina adapta√ß√£o e preserva√ß√£o

**Vantagens**:
- ‚úÖ Plasticidade real (Backpropamine)
- ‚úÖ Preserva√ß√£o (MAS)
- ‚úÖ Balanceamento adapta√ß√£o/preserva√ß√£o
- ‚úÖ MAS j√° implementado

**Desvantagens**:
- ‚ö†Ô∏è Complexidade adicional
- ‚ö†Ô∏è Dois mecanismos para gerenciar
- ‚ö†Ô∏è Pode ser redundante

**Implementa√ß√£o**:
- Backpropamine para mudan√ßas
- MAS para identificar import√¢ncia
- Integra√ß√£o cuidadosa

**Custo Computacional**:
- Mem√≥ria: ~2x (Backpropamine) + overhead MAS
- Computa√ß√£o: ~1.5x

---

### Op√ß√£o D: Differentiable Plasticity (Sem Neuromodula√ß√£o)

**Descri√ß√£o**:
- Usar Differentiable Plasticity b√°sico (sem neuromodula√ß√£o)
- Mais simples que Backpropamine
- Plasticidade real mas sem controle contextual

**Vantagens**:
- ‚úÖ Plasticidade real
- ‚úÖ Mais simples que Backpropamine
- ‚úÖ Menos par√¢metros
- ‚úÖ C√≥digo dispon√≠vel

**Desvantagens**:
- ‚ö†Ô∏è Sem neuromodula√ß√£o (menos controle)
- ‚ö†Ô∏è N√£o t√£o biol√≥gico quanto Backpropamine
- ‚ö†Ô∏è Pode ser menos eficiente

**Implementa√ß√£o**:
- Usar c√≥digo Differentiable Plasticity
- Adaptar para Transformers
- Integrar com sistema

**Custo Computacional**:
- Mem√≥ria: ~2x
- Computa√ß√£o: ~1.3x

---

### Op√ß√£o E: MAS + Fine-tuning Incremental

**Descri√ß√£o**:
- Manter MAS (j√° implementado)
- Fine-tuning incremental com preserva√ß√£o MAS
- N√£o √© plasticidade real, mas funcional

**Vantagens**:
- ‚úÖ MAS j√° implementado
- ‚úÖ Funcional e testado
- ‚úÖ Baixo overhead
- ‚úÖ Pr√°tico

**Desvantagens**:
- ‚ùå N√£o √© plasticidade real
- ‚ùå N√£o alinhado com objetivos (neuroplasticidade)
- ‚ùå Fine-tuning ainda causa algum esquecimento
- ‚ùå N√£o √© revolucion√°rio

**Implementa√ß√£o**:
- Usar MAS existente
- Fine-tuning com preserva√ß√£o
- Consolida√ß√£o peri√≥dica

**Custo Computacional**:
- Mem√≥ria: ~1.1x (overhead MAS)
- Computa√ß√£o: ~1.1x

---

## Recomenda√ß√µes

### Recomenda√ß√£o Principal: **Op√ß√£o C (Backpropamine + MAS H√≠brido)**

**Justificativa**:

1. **Alinhamento com Objetivos**:
   - ‚úÖ Plasticidade real (Backpropamine) - nome do projeto
   - ‚úÖ Preserva√ß√£o (MAS) - j√° implementado
   - ‚úÖ Aprendizado cont√≠nuo verdadeiro

2. **Balanceamento**:
   - Backpropamine: Adapta√ß√£o r√°pida e contextual
   - MAS: Preserva√ß√£o de conhecimento importante
   - Juntos: Melhor dos dois mundos

3. **Viabilidade**:
   - MAS j√° funciona no projeto
   - Backpropamine tem c√≥digo dispon√≠vel
   - Implementa√ß√£o incremental poss√≠vel

4. **Biol√≥gico**:
   - Backpropamine baseado em neuroci√™ncia real
   - MAS identifica import√¢ncia (como c√©rebro)
   - Alinhado com arquitetura biol√≥gica

5. **Extensibilidade**:
   - Base s√≥lida para adicionar outros mecanismos
   - Integra√ß√£o com RAG poss√≠vel
   - Integra√ß√£o com sistema dopamin√©rgico poss√≠vel

**Plano de Implementa√ß√£o**:
1. **Fase 1**: Implementar Backpropamine b√°sico
2. **Fase 2**: Integrar com MAS existente
3. **Fase 3**: Otimizar para LLMs grandes
4. **Fase 4**: Integrar com RAG e consolida√ß√£o

### Recomenda√ß√£o Secund√°ria: **Op√ß√£o B (Backpropamine Seletivo)** se overhead for muito alto

Se o overhead completo for proibitivo, come√ßar com Backpropamine seletivo e expandir depois.

---

## Decis√£o Final

**ESCOLHA PENDENTE - Aguardando confirma√ß√£o do decisor**

### Proposta de Decis√£o:

**Op√ß√£o C: Backpropamine + MAS H√≠brido**

**Justificativa da Escolha**:
- Combina plasticidade real (Backpropamine) com preserva√ß√£o (MAS)
- Alinhado com objetivos do projeto (neuroplasticidade)
- Implementa√ß√£o incremental poss√≠vel
- Base s√≥lida para expans√£o

**Plano de Implementa√ß√£o**:
1. Implementar Backpropamine b√°sico
2. Integrar com MAS existente
3. Testar em modelo pequeno primeiro
4. Escalar para LLM completo
5. Otimizar performance

---

## Implica√ß√µes para o Projeto

### Arquitetura

**Componentes Necess√°rios**:
1. **Backpropamine Layer**:
   - Camada que adiciona pesos pl√°sticos
   - C√°lculo de plasticidade
   - Integra√ß√£o com neuromodula√ß√£o

2. **MAS Integration**:
   - Identifica√ß√£o de import√¢ncia
   - Preserva√ß√£o durante Backpropamine
   - Balanceamento adapta√ß√£o/preserva√ß√£o

3. **Plasticity Manager**:
   - Orquestra√ß√£o de plasticidade
   - Controle de quando/onde aplicar
   - Integra√ß√£o com sistema dopamin√©rgico

### Estrutura de C√≥digo

```
src/brain/plasticity/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ backpropamine.py      # Implementa√ß√£o Backpropamine
‚îú‚îÄ‚îÄ mas_integration.py    # Integra√ß√£o com MAS
‚îú‚îÄ‚îÄ plasticity_manager.py # Gerenciador de plasticidade
‚îî‚îÄ‚îÄ neuromodulation.py    # Neuromodula√ß√£o (futuro)
```

### Recursos Necess√°rios

**Mem√≥ria**:
- Base: ~7GB (LLM 7B quantizado)
- Backpropamine: +7GB (pesos pl√°sticos)
- Total: ~14GB (pode ser otimizado)

**Computa√ß√£o**:
- Overhead: ~50% adicional
- Pode ser otimizado com implementa√ß√£o eficiente

### Integra√ß√£o com Outros Componentes

1. **RAG (Hipocampo)**:
   - Backpropamine consolida mem√≥rias importantes
   - MAS identifica o que consolidar

2. **Sistema Dopamin√©rgico**:
   - Neuromodula√ß√£o controla plasticidade
   - Recompensa guia aprendizado

3. **Consolida√ß√£o Durante "Sono"**:
   - Backpropamine consolida em modelo base
   - MAS preserva conhecimento importante

---

## Pr√≥ximas Decis√µes Dependentes

Esta decis√£o afeta:

1. **NP-002**: Consolida√ß√£o de Mem√≥ria
   - Depende de como Backpropamine funciona

2. **NP-003**: Neuromodula√ß√£o
   - Integra√ß√£o com Backpropamine

3. **NP-006**: Integra√ß√£o Backpropamine + RAG
   - Como integrar plasticidade com mem√≥ria externa

4. **NP-007**: Sistema Dopamin√©rgico
   - Como neuromodula√ß√£o controla plasticidade

---

## Refer√™ncias

### Papers Acad√™micos

1. **Differentiable plasticity: training plastic neural networks with backpropagation**
   - Miconi, T., Clune, J., & Stanley, K. O. (2018)
   - ArXiv: [1804.02464](https://arxiv.org/abs/1804.02464)

2. **Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity**
   - Miconi, T., Rawal, A., Clune, J., & Stanley, K. O. (2020)
   - ArXiv: [2002.10585](https://arxiv.org/abs/2002.10585)

3. **Memory Aware Synapses: Learning what (not) to forget**
   - Aljundi, R., et al. (2017)
   - ArXiv: [1711.09601](https://arxiv.org/abs/1711.09601)

4. **Overcoming catastrophic forgetting in neural networks**
   - Kirkpatrick, J., et al. (2017)
   - PNAS, 114(13), 3521-3526

### Documenta√ß√£o do Projeto

- `docs/01-neuroplasticity-processes/synaptic-plasticity.md` - Documenta√ß√£o completa
- `ARQUITETURA_BIOLOGICA.md` - Arquitetura baseada em neuroci√™ncia
- `src/learning/continual_learning.py` - Implementa√ß√£o MAS atual

### Recursos Online

- Backpropamine GitHub: C√≥digo oficial
- Papers with Code: Differentiable Plasticity
- ArXiv: cs.NE (Neural and Evolutionary Computing)

---

## Notas Adicionais

- **Valida√ß√£o Necess√°ria**: Testar Backpropamine em modelo pequeno primeiro
- **Otimiza√ß√µes**: Pesquisar otimiza√ß√µes espec√≠ficas para Transformers
- **Benchmarks**: Criar benchmarks para medir plasticidade
- **Documenta√ß√£o**: Documentar cada etapa da implementa√ß√£o

---

**Pr√≥ximo Passo**: Aguardar confirma√ß√£o da decis√£o para prosseguir com NP-002 (Consolida√ß√£o de Mem√≥ria).

