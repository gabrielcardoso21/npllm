# Funcionamento do Dia-a-Dia: Arquitetura Completa com Todas as LLMs

**Data**: 2025-01-27  
**Vers√£o**: 2.0 (Revisado - Todas as LLMs)  
**Status**: üìä Arquitetura Completa Revisada

---

## üìã Sum√°rio Executivo

Este documento revisa a **arquitetura completa original** e documenta como o sistema funciona no dia-a-dia, incluindo **todas as LLMs utilizadas**:

1. **LLM Base** (CodeLlama 3B) - C√≥rtex Pr√©-Frontal
2. **Modulador** (1-5M) - C√≥rtex Pr√©-Frontal
3. **Cerebelo** (100M-500M) - Padr√µes Espec√≠ficos
4. **Aten√ß√£o Neuromodulada** - Controle Contextual
5. **LoRA Adapters** - Adapta√ß√£o R√°pida
6. **Modelos Especializados** - Processos Psicol√≥gicos

**Foco**: Aprendizado real e funcionamento di√°rio, sem integra√ß√£o Linux.

---

## üß† Todas as LLMs Utilizadas no Sistema

### 1. LLM Base (CodeLlama 3B) - C√≥rtex Pr√©-Frontal (PFC)

**Fun√ß√£o**: Racioc√≠nio principal, planejamento, controle executivo

**Caracter√≠sticas**:
- ‚úÖ **Modelo**: CodeLlama 3B quantizado 4-bit
- ‚úÖ **Tamanho**: ~3 bilh√µes de par√¢metros
- ‚úÖ **Status**: Est√°tico (n√£o muda durante uso)
- ‚úÖ **Atualiza√ß√£o**: Apenas durante "sono" (consolida√ß√£o)
- ‚úÖ **Uso**: Processamento principal de c√≥digo e arquitetura

**Onde √© Usado**:
- Gera√ß√£o de c√≥digo arquitetural
- An√°lise de padr√µes
- Sugest√µes arquiteturais
- Racioc√≠nio e planejamento
- Processos psicol√≥gicos (pensamento, linguagem, resolu√ß√£o de problemas)

**Integra√ß√£o**:
- Recebe contexto do RAG (Hipocampo)
- Usa LoRA Adapters para adapta√ß√£o r√°pida
- Modulado pelo Modulador (sele√ß√£o de adapters)
- Controlado por Aten√ß√£o Neuromodulada

---

### 2. Modulador (1-5M par√¢metros) - C√≥rtex Pr√©-Frontal (PFC)

**Fun√ß√£o**: Sele√ß√£o e modula√ß√£o de adapters baseado em contexto

**Caracter√≠sticas**:
- ‚úÖ **Modelo**: Pequeno (1-5M par√¢metros)
- ‚úÖ **Tamanho**: Muito menor que LLM Base
- ‚úÖ **Status**: Pode usar Backpropamine (experimental)
- ‚úÖ **Atualiza√ß√£o**: Durante uso (se Backpropamine) ou durante sono
- ‚úÖ **Uso**: Decis√£o de qual adapter usar

**Onde √© Usado**:
- An√°lise de hidden states do LLM Base
- Gera√ß√£o de sinais de modula√ß√£o
- Sele√ß√£o de adapters apropriados
- Controle de intensidade de adapta√ß√£o
- Integra√ß√£o com RL (PPO)

**Integra√ß√£o**:
- Recebe hidden states do LLM Base (bottom-up)
- Gera sinais de modula√ß√£o (top-down)
- Controla LoRA Adapters
- Treinado com RL (PPO)
- Usa Backpropamine para aprendizado r√°pido (experimental)

---

### 3. Cerebelo (100M-500M par√¢metros) - Padr√µes Espec√≠ficos

**Fun√ß√£o**: Padr√µes espec√≠ficos, automatiza√ß√£o, aprendizado r√°pido

**Caracter√≠sticas**:
- ‚ö†Ô∏è **Modelo**: M√©dio (100M-500M par√¢metros)
- ‚ö†Ô∏è **Status**: Experimental (Fase 2)
- ‚ö†Ô∏è **Atualiza√ß√£o**: Backpropamine (mudan√ßas reais de pesos)
- ‚ö†Ô∏è **Uso**: Padr√µes espec√≠ficos, automatiza√ß√£o

**Onde √© Usado**:
- Padr√µes arquiteturais espec√≠ficos frequentes
- Automatiza√ß√£o de tarefas repetitivas
- Aprendizado r√°pido de novos padr√µes
- Padr√µes que geram satisfa√ß√£o (priorizados)

**Integra√ß√£o**:
- Recebe padr√µes do sistema de mem√≥ria
- Aprende rapidamente com Backpropamine
- Consolida durante sono
- Transfere conhecimento importante para LLM Base

---

### 4. Aten√ß√£o Neuromodulada - Controle Contextual

**Fun√ß√£o**: Modular onde focar baseado em contexto e feedback

**Caracter√≠sticas**:
- ‚ö†Ô∏è **Modelo**: N√£o √© modelo separado, mas mecanismo de aten√ß√£o
- ‚ö†Ô∏è **Status**: Experimental (Fase 2)
- ‚ö†Ô∏è **Atualiza√ß√£o**: Backpropamine (mudan√ßas reais de pesos de aten√ß√£o)
- ‚ö†Ô∏è **Uso**: Controle contextual de aten√ß√£o

**Onde √© Usado**:
- Modular aten√ß√£o do LLM Base
- Focar em partes relevantes do contexto
- Filtrar informa√ß√£o irrelevante
- Priorizar conhecimento importante

**Integra√ß√£o**:
- Integrado com mecanismo de aten√ß√£o do LLM Base
- Controlado por feedback emocional
- Usa Backpropamine para adapta√ß√£o r√°pida
- Consolida durante sono

---

### 5. LoRA Adapters (pesos adicionais) - Adapta√ß√£o R√°pida

**Fun√ß√£o**: Adapta√ß√£o r√°pida a contextos espec√≠ficos

**Caracter√≠sticas**:
- ‚úÖ **Modelo**: Pesos adicionais (n√£o modelo completo)
- ‚úÖ **Tamanho**: Muito menor que modelo base
- ‚úÖ **Status**: M√∫ltiplos adapters por contexto
- ‚úÖ **Atualiza√ß√£o**: Durante uso (treinamento incremental)
- ‚úÖ **Uso**: Especializa√ß√£o por contexto (ex: Odoo, Django, React)

**Onde √© Usado**:
- Adapta√ß√£o a contextos espec√≠ficos
- Especializa√ß√£o por projeto
- Aprendizado r√°pido sem mudar modelo base
- M√∫ltiplos adapters simult√¢neos

**Integra√ß√£o**:
- Aplicados ao LLM Base
- Selecionados pelo Modulador
- Treinados com feedback emocional
- Consolidados durante sono

---

### 6. Modelos Especializados (Futuro) - Processos Psicol√≥gicos

**Fun√ß√£o**: Processos psicol√≥gicos espec√≠ficos

**Caracter√≠sticas**:
- ‚ö†Ô∏è **Modelo**: Modelos pequenos especializados
- ‚ö†Ô∏è **Status**: Planejado (Fase 3)
- ‚ö†Ô∏è **Uso**: Processos psicol√≥gicos espec√≠ficos

**Onde Ser√£o Usados**:
- Percep√ß√£o (pattern recognition)
- Emo√ß√£o (an√°lise emocional)
- Metacogni√ß√£o (monitoramento)
- Criatividade (explora√ß√£o)

**Integra√ß√£o**:
- Integrados com LLM Base
- Usam feedback emocional
- Consolidam durante sono

---

## üèóÔ∏è Arquitetura Completa: Todas as LLMs

```mermaid
graph TB
    subgraph "Entrada"
        USER[Usu√°rio<br/>Query/C√≥digo/Feedback]
    end
    
    subgraph "Mem√≥ria"
        CACHE[Cache R√°pido<br/>Mem√≥ria Curta]
        WORKING[Mem√≥ria Trabalho<br/>Contexto Atual]
        POSTGRES[PostgreSQL + pgvector<br/>Hipocampo<br/>Mem√≥ria M√©dia]
    end
    
    subgraph "LLMs Principais"
        LLM_BASE[LLM Base<br/>CodeLlama 3B<br/>PFC - Racioc√≠nio]
        MODULATOR[Modulador<br/>1-5M par√¢metros<br/>Sele√ß√£o Adapters]
        CEREBELO[Cerebelo*<br/>100M-500M<br/>Padr√µes Espec√≠ficos]
        ATTENTION[Aten√ß√£o Neuromodulada*<br/>Controle Contextual]
    end
    
    subgraph "Adapta√ß√£o"
        LORA[LoRA Adapters<br/>Adapta√ß√£o R√°pida<br/>M√∫ltiplos Contextos]
    end
    
    subgraph "Aprendizado Real"
        MAS[MAS<br/>Preserva√ß√£o]
        REPLAY[Replay Buffer<br/>Mem√≥rias Importantes]
        BACKPROP[Backpropamine*<br/>Plasticidade Real]
        RL[RL PPO<br/>Sistema Dopamin√©rgico]
    end
    
    subgraph "Feedback"
        IMPLICIT[Feedback Impl√≠cito<br/>70%]
        EMOTIONAL[Feedback Emocional<br/>30%]
        INTEGRATE[Integra√ß√£o Feedback]
    end
    
    subgraph "Consolida√ß√£o"
        SLEEP[Consolida√ß√£o Durante Sono<br/>Hipocampo ‚Üí C√≥rtex]
        FT[Fine-tuning<br/>Incremental]
    end
    
    USER --> CACHE
    CACHE --> POSTGRES
    POSTGRES --> WORKING
    WORKING --> LLM_BASE
    
    LLM_BASE --> MODULATOR
    MODULATOR --> LORA
    LORA --> LLM_BASE
    
    LLM_BASE --> ATTENTION
    ATTENTION --> LLM_BASE
    
    POSTGRES --> CEREBELO
    CEREBELO --> POSTGRES
    
    USER --> IMPLICIT
    USER --> EMOTIONAL
    IMPLICIT --> INTEGRATE
    EMOTIONAL --> INTEGRATE
    INTEGRATE --> REPLAY
    INTEGRATE --> RL
    INTEGRATE --> BACKPROP
    
    REPLAY --> POSTGRES
    RL --> MODULATOR
    RL --> BACKPROP
    BACKPROP --> MODULATOR
    BACKPROP --> CEREBELO
    BACKPROP --> ATTENTION
    BACKPROP --> MAS
    
    MAS --> LLM_BASE
    MAS --> LORA
    
    POSTGRES --> SLEEP
    SLEEP --> FT
    FT --> LLM_BASE
    FT --> LORA
    FT --> CEREBELO
    
    LLM_BASE --> USER
    
    style LLM_BASE fill:#e1f5ff
    style MODULATOR fill:#fff4e1
    style CEREBELO fill:#f0e1ff
    style ATTENTION fill:#ffe1f5
    style LORA fill:#ffe1f5
    style POSTGRES fill:#ccffcc
    style MAS fill:#ffcccc
    style REPLAY fill:#ffffcc
    style BACKPROP fill:#ffcccc
    style RL fill:#e1ffe1
    style INTEGRATE fill:#ffccff
    style SLEEP fill:#ccccff
```

**Legenda**:
- `*` = Componente experimental (Backpropamine, Cerebelo, Aten√ß√£o Neuromodulada)
- Cores diferentes = Diferentes subsistemas

---

## üîÑ Funcionamento do Dia-a-Dia: Fluxo Completo

### Manh√£: Primeiras Intera√ß√µes

```mermaid
sequenceDiagram
    participant USER as Usu√°rio
    participant CACHE as Cache R√°pido
    participant POSTGRES as PostgreSQL<br/>(Hipocampo)
    participant LLM as LLM Base<br/>(PFC)
    participant MOD as Modulador
    participant LORA as LoRA Adapters
    participant ATT as Aten√ß√£o<br/>Neuromodulada
    participant CEREB as Cerebelo
    
    USER->>CACHE: Query sobre arquitetura
    CACHE->>POSTGRES: Busca sem√¢ntica (se n√£o encontrar)
    POSTGRES-->>CACHE: Resultados relevantes
    CACHE->>LLM: Contexto + Query
    
    LLM->>MOD: Hidden states
    MOD->>MOD: Analisa contexto
    MOD->>LORA: Seleciona adapters apropriados
    LORA->>LLM: Adapta√ß√£o aplicada
    
    LLM->>ATT: Hidden states
    ATT->>ATT: Modular aten√ß√£o (focar no relevante)
    ATT->>LLM: Aten√ß√£o modulada
    
    LLM->>CEREB: Padr√µes identificados
    CEREB->>CEREB: Verifica padr√µes conhecidos
    CEREB-->>LLM: Padr√µes relevantes
    
    LLM->>USER: Resposta/Sugest√£o Arquitetural
```

**O Que Acontece**:
1. Usu√°rio faz query sobre arquitetura
2. Cache verifica se tem resposta
3. Se n√£o, PostgreSQL busca sem√¢ntica
4. LLM Base processa com contexto
5. Modulador seleciona adapters apropriados
6. Aten√ß√£o Neuromodulada foca no relevante
7. Cerebelo verifica padr√µes conhecidos
8. Resposta √© gerada e apresentada

---

### Durante o Dia: Aprendizado Cont√≠nuo

```mermaid
sequenceDiagram
    participant USER as Usu√°rio
    participant FEEDBACK as Feedback<br/>(Emocional + Impl√≠cito)
    participant REPLAY as Replay Buffer
    participant RL as RL PPO
    participant BACKPROP as Backpropamine
    participant MOD as Modulador
    participant CEREB as Cerebelo
    participant ATT as Aten√ß√£o<br/>Neuromodulada
    participant POSTGRES as PostgreSQL
    participant MAS as MAS
    
    USER->>FEEDBACK: Emo√ß√£o (frustra√ß√£o/satisfa√ß√£o/confian√ßa)
    USER->>FEEDBACK: A√ß√£o (aceitar/editar/deletar)
    FEEDBACK->>FEEDBACK: Integra (70% impl√≠cito + 30% emocional)
    
    FEEDBACK->>REPLAY: Feedback integrado
    REPLAY->>REPLAY: Avalia import√¢ncia<br/>(Prioriza satisfa√ß√£o)
    REPLAY->>POSTGRES: Persiste conhecimento importante
    
    FEEDBACK->>RL: Recompensa (feedback integrado)
    RL->>MOD: Atualiza pol√≠tica (PPO)
    RL->>BACKPROP: Sinal de recompensa
    
    BACKPROP->>MOD: Atualiza pesos (plasticidade real)
    BACKPROP->>CEREB: Atualiza pesos (plasticidade real)
    BACKPROP->>ATT: Atualiza pesos de aten√ß√£o (plasticidade real)
    
    BACKPROP->>MAS: Preserva conhecimento antigo importante
    MAS->>LLM: Protege conhecimento consolidado
```

**O Que Acontece**:
1. Usu√°rio interage, recebe sugest√µes
2. Feedback √© capturado (emocional + impl√≠cito)
3. Replay Buffer avalia import√¢ncia (prioriza satisfa√ß√£o)
4. Conhecimento importante √© persistido no PostgreSQL
5. RL PPO atualiza pol√≠tica do Modulador
6. Backpropamine atualiza Modulador, Cerebelo e Aten√ß√£o
7. MAS preserva conhecimento antigo importante

---

### Noite: Consolida√ß√£o (Sono)

```mermaid
sequenceDiagram
    participant POSTGRES as PostgreSQL<br/>(Hipocampo)
    participant REPLAY as Replay Buffer
    participant MAS as MAS
    participant FT as Fine-tuning
    participant LLM as LLM Base<br/>(PFC)
    participant LORA as LoRA Adapters
    participant CEREB as Cerebelo
    
    Note over POSTGRES,CEREB: Durante "Sono" (Consolida√ß√£o)
    
    POSTGRES->>REPLAY: Conhecimento acumulado
    REPLAY->>REPLAY: Filtra por feedback emocional<br/>(Score > 0.7, prioriza satisfa√ß√£o)
    REPLAY->>MAS: Conhecimento importante
    
    MAS->>MAS: Calcula import√¢ncia de par√¢metros
    MAS->>MAS: Preserva conhecimento antigo importante
    
    MAS->>FT: Dataset de treinamento filtrado
    FT->>FT: Fine-tuning com MAS<br/>(Preserva conhecimento antigo)
    
    FT->>LLM: Atualiza pesos do modelo base
    FT->>LORA: Consolida adapters
    FT->>CEREB: Consolida padr√µes importantes
    
    Note over LLM,CEREB: Consolida√ß√£o Completa
```

**O Que Acontece**:
1. Sistema detecta inatividade
2. PostgreSQL acumulou conhecimento suficiente
3. Replay Buffer filtra por feedback emocional (prioriza satisfa√ß√£o)
4. MAS preserva conhecimento antigo importante
5. Fine-tuning consolida conhecimento
6. Pesos da LLM Base s√£o atualizados
7. LoRA Adapters s√£o consolidados
8. Cerebelo consolida padr√µes importantes

---

### Pr√≥ximo Dia: Conhecimento Consolidado

**O Que Mudou**:
1. **LLM Base** tem conhecimento consolidado
2. **LoRA Adapters** est√£o atualizados
3. **Modulador** aprendeu padr√µes de sele√ß√£o
4. **Cerebelo** tem padr√µes importantes consolidados
5. **Aten√ß√£o Neuromodulada** foca melhor no relevante
6. **Sistema** est√° mais inteligente

---

## üìä Tabela: Todas as LLMs e Suas Fun√ß√µes

| LLM | Tamanho | Fun√ß√£o | Onde Usado | Atualiza√ß√£o | Status |
|-----|---------|--------|------------|-------------|--------|
| **LLM Base** | 3B | Racioc√≠nio principal | PFC, processamento principal | Durante sono | ‚úÖ Implementado |
| **Modulador** | 1-5M | Sele√ß√£o de adapters | PFC, controle de adapters | Durante uso (Backpropamine) ou sono | ‚úÖ Implementado |
| **Cerebelo** | 100M-500M | Padr√µes espec√≠ficos | Automatiza√ß√£o, padr√µes | Backpropamine (experimental) | ‚ö†Ô∏è Experimental |
| **Aten√ß√£o Neuromodulada** | Mecanismo | Controle contextual | Aten√ß√£o do LLM Base | Backpropamine (experimental) | ‚ö†Ô∏è Experimental |
| **LoRA Adapters** | Pesos adicionais | Adapta√ß√£o r√°pida | Especializa√ß√£o por contexto | Durante uso e sono | ‚úÖ Implementado |
| **Modelos Especializados** | Pequenos | Processos psicol√≥gicos | Percep√ß√£o, emo√ß√£o, etc. | Durante sono | ‚ö†Ô∏è Planejado |

**Total**: 3-4 modelos principais + m√∫ltiplos adapters + mecanismos especializados

---

## üéØ Resumo: Como Funciona no Dia-a-Dia

### Ciclo Completo

```
Manh√£: Intera√ß√£o
  ‚Üì
Durante o Dia: Aprendizado Cont√≠nuo
  ‚Üì
Noite: Consolida√ß√£o (Sono)
  ‚Üì
Pr√≥ximo Dia: Conhecimento Consolidado
  ‚Üì
Repete...
```

### Componentes em A√ß√£o

1. **LLM Base**: Processamento principal, racioc√≠nio
2. **Modulador**: Sele√ß√£o inteligente de adapters
3. **Cerebelo**: Padr√µes espec√≠ficos, automatiza√ß√£o
4. **Aten√ß√£o Neuromodulada**: Foco no relevante
5. **LoRA Adapters**: Adapta√ß√£o r√°pida por contexto
6. **Backpropamine**: Aprendizado real (experimental)
7. **MAS**: Preserva√ß√£o de conhecimento
8. **Replay**: Prioriza√ß√£o de satisfa√ß√£o
9. **Feedback Emocional**: Guia aprendizado

### Fluxo de Aprendizado

```
Intera√ß√£o ‚Üí Feedback ‚Üí Aprendizado ‚Üí Consolida√ß√£o ‚Üí Melhoria
    ‚Üì           ‚Üì            ‚Üì              ‚Üì            ‚Üì
  Query    Emo√ß√£o +    Backpropamine    Sono      LLM Mais
  C√≥digo   A√ß√£o        + MAS + Replay            Inteligente
```

---

## üî¨ Detalhamento: Como Cada LLM Aprende

### LLM Base (CodeLlama 3B)

**Aprendizado**:
- ‚ö†Ô∏è **N√£o durante uso**: Est√°tico, n√£o muda
- ‚úÖ **Durante sono**: Fine-tuning incremental com MAS
- ‚úÖ **Preserva√ß√£o**: MAS protege conhecimento antigo

**Quando Aprende**:
- Durante consolida√ß√£o (sono)
- Conhecimento importante do PostgreSQL
- Filtrado por feedback emocional

---

### Modulador (1-5M)

**Aprendizado**:
- ‚úÖ **Durante uso**: Backpropamine (experimental)
- ‚úÖ **Durante sono**: Fine-tuning com RL (PPO)
- ‚úÖ **Preserva√ß√£o**: MAS protege conhecimento antigo

**Quando Aprende**:
- A cada intera√ß√£o (se Backpropamine ativo)
- Durante consolida√ß√£o (sono)
- Baseado em feedback emocional + impl√≠cito

---

### Cerebelo (100M-500M)

**Aprendizado**:
- ‚úÖ **Durante uso**: Backpropamine (mudan√ßas reais)
- ‚úÖ **Durante sono**: Consolida√ß√£o de padr√µes
- ‚úÖ **Preserva√ß√£o**: MAS protege padr√µes importantes

**Quando Aprende**:
- A cada padr√£o identificado (se Backpropamine ativo)
- Durante consolida√ß√£o (sono)
- Padr√µes que geram satisfa√ß√£o (priorizados)

---

### Aten√ß√£o Neuromodulada

**Aprendizado**:
- ‚úÖ **Durante uso**: Backpropamine (mudan√ßas reais)
- ‚úÖ **Durante sono**: Consolida√ß√£o de padr√µes de aten√ß√£o
- ‚úÖ **Preserva√ß√£o**: MAS protege padr√µes importantes

**Quando Aprende**:
- A cada intera√ß√£o (se Backpropamine ativo)
- Durante consolida√ß√£o (sono)
- Baseado em feedback emocional (focar no que gera satisfa√ß√£o)

---

### LoRA Adapters

**Aprendizado**:
- ‚úÖ **Durante uso**: Treinamento incremental
- ‚úÖ **Durante sono**: Consolida√ß√£o
- ‚úÖ **Preserva√ß√£o**: MAS protege conhecimento importante

**Quando Aprende**:
- A cada intera√ß√£o (treinamento incremental)
- Durante consolida√ß√£o (sono)
- Baseado em feedback emocional + impl√≠cito

---

## üéØ Conclus√£o: Funcionamento do Dia-a-Dia

### Arquitetura Completa com Todas as LLMs

O sistema npllm utiliza:

1. **3-4 modelos principais**:
   - LLM Base (3B) - Racioc√≠nio principal
   - Modulador (1-5M) - Sele√ß√£o de adapters
   - Cerebelo (100M-500M) - Padr√µes espec√≠ficos (experimental)
   - Aten√ß√£o Neuromodulada - Controle contextual (experimental)

2. **M√∫ltiplos LoRA Adapters**:
   - Adapta√ß√£o r√°pida por contexto
   - Especializa√ß√£o por projeto

3. **Sistema de aprendizado integrado**:
   - MAS (preserva√ß√£o)
   - Replay (mem√≥rias importantes, prioriza satisfa√ß√£o)
   - Backpropamine (plasticidade real - experimental)
   - RL PPO (sistema dopamin√©rgico)

4. **Sistema de feedback emocional**:
   - Feedback impl√≠cito (70%)
   - Feedback emocional (30%)
   - Integra√ß√£o para prioriza√ß√£o

5. **Sistema de consolida√ß√£o**:
   - Durante sono
   - Filtragem por feedback emocional
   - Transfer√™ncia para pesos de todas as LLMs

### Funcionamento Di√°rio

- **Manh√£**: Intera√ß√µes, processamento com todas as LLMs
- **Durante o Dia**: Aprendizado cont√≠nuo, Backpropamine atualiza modelos pequenos
- **Noite**: Consolida√ß√£o, conhecimento vai para todas as LLMs
- **Pr√≥ximo Dia**: Sistema mais inteligente, todas as LLMs melhoradas

---

**Data de Cria√ß√£o**: 2025-01-27  
**√öltima Atualiza√ß√£o**: 2025-01-27  
**Status**: ‚úÖ Completo - Arquitetura Completa Revisada com Todas as LLMs

