# Funcionamento do Dia-a-Dia: Arquitetura Completa com Todas as LLMs

**Data**: 2025-01-27  
**Vers√£o**: 2.0 (Revisado - Todas as LLMs)  
**Status**: üìä Arquitetura Completa Revisada

---

## üìã Sum√°rio Executivo

Este documento revisa a **arquitetura completa original** e documenta como o sistema funciona no dia-a-dia, incluindo **todas as LLMs utilizadas**:

1. **LLM Base** (CodeLlama 3B) - C√≥rtex Pr√©-Frontal
2. **Modulador** (1-5M) - C√≥rtex Pr√©-Frontal
3. **Cerebelo** (100M-500M) - Padr√µes Espec√≠ficos
4. **Aten√ß√£o Neuromodulada** - Controle Contextual
5. **LoRA Adapters** - Adapta√ß√£o R√°pida
6. **Modelos Especializados** - Processos Psicol√≥gicos

**Foco**: Aprendizado real e funcionamento di√°rio, sem integra√ß√£o Linux.

---

## üß† Todas as LLMs Utilizadas no Sistema

### 1. LLM Base (CodeLlama 3B) - C√≥rtex Pr√©-Frontal (PFC)

**Fun√ß√£o**: Racioc√≠nio principal, planejamento, controle executivo

**Caracter√≠sticas**:
- ‚úÖ **Modelo**: CodeLlama 3B quantizado 4-bit
- ‚úÖ **Tamanho**: ~3 bilh√µes de par√¢metros
- ‚úÖ **Status**: Est√°tico (n√£o muda nunca)
- ‚ùå **Atualiza√ß√£o**: N√ÉO √© treinada (plug-and-play)
- ‚úÖ **Uso**: Processamento principal de c√≥digo e arquitetura
- ‚úÖ **Pode ser trocada**: Por qualquer LLM compat√≠vel sem perder conhecimento

**Onde √© Usado**:
- Gera√ß√£o de c√≥digo arquitetural
- An√°lise de padr√µes
- Sugest√µes arquiteturais
- Racioc√≠nio e planejamento
- Processos psicol√≥gicos (pensamento, linguagem, resolu√ß√£o de problemas)

**Integra√ß√£o**:
- Recebe contexto do RAG (Hipocampo)
- Usa LoRA Adapters para adapta√ß√£o r√°pida
- Modulado pelo Modulador (sele√ß√£o de adapters)
- Controlado por Aten√ß√£o Neuromodulada

---

### 2. Modulador (1-5M par√¢metros) - C√≥rtex Pr√©-Frontal (PFC)

**Fun√ß√£o**: Sele√ß√£o e modula√ß√£o de adapters baseado em contexto

**Caracter√≠sticas**:
- ‚úÖ **Modelo**: Pequeno (1-5M par√¢metros)
- ‚úÖ **Tamanho**: Muito menor que LLM Base
- ‚ö†Ô∏è **Status**: Pode funcionar apenas com infer√™ncia
- ‚ö†Ô∏è **Atualiza√ß√£o**: Opcional, apenas durante sono (se necess√°rio)
- ‚úÖ **Uso**: Decis√£o de qual adapter usar

**Onde √© Usado**:
- An√°lise de hidden states do LLM Base
- Gera√ß√£o de sinais de modula√ß√£o
- Sele√ß√£o de adapters apropriados
- Controle de intensidade de adapta√ß√£o
- Integra√ß√£o com RL (PPO)

**Integra√ß√£o**:
- Recebe hidden states do LLM Base (bottom-up)
- Gera sinais de modula√ß√£o (top-down)
- Controla LoRA Adapters
- Pode ser treinado com RL (PPO) apenas durante sono (opcional)
- N√£o treina durante uso (apenas infer√™ncia)

---

### 3. Cerebelo (100M-500M par√¢metros) - Padr√µes Espec√≠ficos

**Fun√ß√£o**: Padr√µes espec√≠ficos, automatiza√ß√£o, aprendizado r√°pido

**Caracter√≠sticas**:
- ‚ö†Ô∏è **Modelo**: M√©dio (100M-500M par√¢metros)
- ‚ö†Ô∏è **Status**: Experimental (Fase 2)
- ‚úÖ **Atualiza√ß√£o**: Essencial treinar, mas apenas durante sono
- ‚ö†Ô∏è **Uso**: Padr√µes espec√≠ficos, automatiza√ß√£o

**Onde √© Usado**:
- Padr√µes arquiteturais espec√≠ficos frequentes
- Automatiza√ß√£o de tarefas repetitivas
- Aprendizado r√°pido de novos padr√µes
- Padr√µes que geram satisfa√ß√£o (priorizados)

**Integra√ß√£o**:
- Recebe padr√µes do sistema de mem√≥ria
- Aprende apenas durante sono (Backpropamine + consolida√ß√£o)
- N√£o treina durante uso (evita overhead)
- Conhecimento importante √© consolidado durante sono

---

### 4. Aten√ß√£o Neuromodulada - Controle Contextual

**Fun√ß√£o**: Modular onde focar baseado em contexto e feedback

**Caracter√≠sticas**:
- ‚ö†Ô∏è **Modelo**: N√£o √© modelo separado, mas mecanismo de aten√ß√£o
- ‚ö†Ô∏è **Status**: Experimental (Fase 2)
- ‚ö†Ô∏è **Atualiza√ß√£o**: Opcional, apenas durante sono (se necess√°rio)
- ‚ö†Ô∏è **Uso**: Controle contextual de aten√ß√£o (pode usar aten√ß√£o padr√£o)

**Onde √© Usado**:
- Modular aten√ß√£o do LLM Base
- Focar em partes relevantes do contexto
- Filtrar informa√ß√£o irrelevante
- Priorizar conhecimento importante

**Integra√ß√£o**:
- Integrado com mecanismo de aten√ß√£o do LLM Base
- Controlado por feedback emocional
- Pode usar Backpropamine apenas durante sono (opcional)
- N√£o treina durante uso (evita overhead)

---

### 5. LoRA Adapters (pesos adicionais) - Adapta√ß√£o R√°pida

**Fun√ß√£o**: Adapta√ß√£o r√°pida a contextos espec√≠ficos

**Caracter√≠sticas**:
- ‚úÖ **Modelo**: Pesos adicionais (n√£o modelo completo)
- ‚úÖ **Tamanho**: Muito menor que modelo base
- ‚úÖ **Status**: M√∫ltiplos adapters por contexto
- ‚úÖ **Atualiza√ß√£o**: Essencial treinar, mas apenas durante sono
- ‚úÖ **Uso**: Especializa√ß√£o por contexto (ex: Odoo, Django, React)

**Onde √© Usado**:
- Adapta√ß√£o a contextos espec√≠ficos
- Especializa√ß√£o por projeto
- Aprendizado r√°pido sem mudar modelo base
- M√∫ltiplos adapters simult√¢neos

**Integra√ß√£o**:
- Aplicados ao LLM Base
- Selecionados pelo Modulador
- Treinados apenas durante sono (com feedback emocional)
- N√£o treinam durante uso (evita overhead)

---

### 6. Modelos Especializados (Futuro) - Processos Psicol√≥gicos

**Fun√ß√£o**: Processos psicol√≥gicos espec√≠ficos

**Caracter√≠sticas**:
- ‚ö†Ô∏è **Modelo**: Modelos pequenos especializados
- ‚ö†Ô∏è **Status**: Planejado (Fase 3)
- ‚ö†Ô∏è **Uso**: Processos psicol√≥gicos espec√≠ficos

**Onde Ser√£o Usados**:
- Percep√ß√£o (pattern recognition)
- Emo√ß√£o (an√°lise emocional)
- Metacogni√ß√£o (monitoramento)
- Criatividade (explora√ß√£o)

**Integra√ß√£o**:
- Integrados com LLM Base
- Usam feedback emocional
- Consolidam durante sono

---

## üèóÔ∏è Arquitetura Completa: Todas as LLMs

```mermaid
graph TB
    subgraph "Entrada"
        USER[Usu√°rio<br/>Query/C√≥digo/Feedback]
    end
    
    subgraph "Mem√≥ria"
        CACHE[Cache R√°pido<br/>Mem√≥ria Curta]
        WORKING[Mem√≥ria Trabalho<br/>Contexto Atual]
        POSTGRES[PostgreSQL + pgvector<br/>Hipocampo<br/>Mem√≥ria M√©dia]
    end
    
    subgraph "LLMs Principais"
        LLM_BASE[LLM Base<br/>CodeLlama 3B<br/>PFC - Racioc√≠nio<br/>Nao Treinada]
        MODULATOR[Modulador<br/>1-5M par√¢metros<br/>Sele√ß√£o Adapters<br/>Opcional]
        CEREBELO[Cerebelo*<br/>100M-500M<br/>Padr√µes Espec√≠ficos<br/>Essencial]
        ATTENTION[Aten√ß√£o Neuromodulada*<br/>Controle Contextual<br/>Opcional]
    end
    
    subgraph "Adapta√ß√£o"
        LORA[LoRA Adapters<br/>Adapta√ß√£o R√°pida<br/>M√∫ltiplos Contextos<br/>Essencial]
    end
    
    subgraph "Aprendizado Real"
        MAS[MAS<br/>Preserva√ß√£o<br/>Durante Sono]
        REPLAY[Replay Buffer<br/>Mem√≥rias Importantes<br/>Coleta Durante Uso]
        BACKPROP[Backpropamine*<br/>Plasticidade Real<br/>Apenas Durante Sono]
        RL[RL PPO<br/>Sistema Dopamin√©rgico<br/>Apenas Durante Sono]
    end
    
    subgraph "Feedback"
        IMPLICIT[Feedback Impl√≠cito<br/>70%]
        EMOTIONAL[Feedback Emocional<br/>30%]
        INTEGRATE[Integra√ß√£o Feedback]
    end
    
    subgraph "Consolida√ß√£o Apenas Durante Sono"
        SLEEP[Consolida√ß√£o Durante Sono<br/>Hipocampo para Cerebelo/LoRA]
        FT[Fine-tuning Incremental<br/>Cerebelo + LoRA<br/>Modulador + Aten√ß√£o opcional]
    end
    
    USER --> CACHE
    CACHE --> POSTGRES
    POSTGRES --> WORKING
    WORKING --> LLM_BASE
    
    LLM_BASE --> MODULATOR
    MODULATOR --> LORA
    LORA --> LLM_BASE
    
    LLM_BASE --> ATTENTION
    ATTENTION --> LLM_BASE
    
    POSTGRES --> CEREBELO
    CEREBELO --> POSTGRES
    
    USER --> IMPLICIT
    USER --> EMOTIONAL
    IMPLICIT --> INTEGRATE
    EMOTIONAL --> INTEGRATE
    INTEGRATE --> REPLAY
    
    REPLAY --> POSTGRES
    
    POSTGRES --> SLEEP
    SLEEP --> FT
    FT --> CEREBELO
    FT --> LORA
    FT --> MODULATOR
    FT --> ATTENTION
    
    MAS --> FT
    RL --> FT
    BACKPROP --> FT
    
    Note over FT: LLM Base nao √© treinada<br/>plug-and-play
    Note over RL,BACKPROP: RL e Backpropamine<br/>s√≥ durante sono
    Note over REPLAY: Durante uso: Apenas coleta feedback<br/>Sem treinamento de modelos
    
    LLM_BASE --> USER
    
    style LLM_BASE fill:#ffcccc
    style MODULATOR fill:#fff4e1
    style CEREBELO fill:#ccffcc
    style ATTENTION fill:#fff4e1
    style LORA fill:#ccffcc
    style POSTGRES fill:#ccffcc
    style MAS fill:#ccccff
    style REPLAY fill:#ffffcc
    style BACKPROP fill:#ccccff
    style RL fill:#ccccff
    style INTEGRATE fill:#ffccff
    style SLEEP fill:#ccccff
    style FT fill:#ccccff
```

**Legenda**:
- `*` = Componente experimental (Backpropamine, Cerebelo, Aten√ß√£o Neuromodulada)
- Cores diferentes = Diferentes subsistemas

---

## üîÑ Funcionamento do Dia-a-Dia: Fluxo Completo

### Manh√£: Primeiras Intera√ß√µes

```mermaid
sequenceDiagram
    participant USER as Usu√°rio
    participant CACHE as Cache R√°pido
    participant POSTGRES as PostgreSQL<br/>(Hipocampo)
    participant LLM as LLM Base<br/>(PFC)
    participant MOD as Modulador
    participant LORA as LoRA Adapters
    participant ATT as Aten√ß√£o<br/>Neuromodulada
    participant CEREB as Cerebelo
    
    USER->>CACHE: Query sobre arquitetura
    CACHE->>POSTGRES: Busca sem√¢ntica (se n√£o encontrar)
    POSTGRES-->>CACHE: Resultados relevantes
    CACHE->>LLM: Contexto + Query
    
    LLM->>MOD: Hidden states
    MOD->>MOD: Analisa contexto
    MOD->>LORA: Seleciona adapters apropriados
    LORA->>LLM: Adapta√ß√£o aplicada
    
    LLM->>ATT: Hidden states
    ATT->>ATT: Modular aten√ß√£o (focar no relevante)
    ATT->>LLM: Aten√ß√£o modulada
    
    LLM->>CEREB: Padr√µes identificados
    CEREB->>CEREB: Verifica padr√µes conhecidos
    CEREB-->>LLM: Padr√µes relevantes
    
    LLM->>USER: Resposta/Sugest√£o Arquitetural
```

**O Que Acontece**:
1. Usu√°rio faz query sobre arquitetura
2. Cache verifica se tem resposta
3. Se n√£o, PostgreSQL busca sem√¢ntica
4. LLM Base processa com contexto
5. Modulador seleciona adapters apropriados
6. Aten√ß√£o Neuromodulada foca no relevante
7. Cerebelo verifica padr√µes conhecidos
8. Resposta √© gerada e apresentada

---

### Durante o Dia: Aprendizado Cont√≠nuo

```mermaid
sequenceDiagram
    participant USER as Usu√°rio
    participant FEEDBACK as Feedback<br/>(Emocional + Impl√≠cito)
    participant REPLAY as Replay Buffer
    participant RL as RL PPO
    participant BACKPROP as Backpropamine
    participant MOD as Modulador
    participant CEREB as Cerebelo
    participant ATT as Aten√ß√£o<br/>Neuromodulada
    participant POSTGRES as PostgreSQL
    participant MAS as MAS
    
    USER->>FEEDBACK: Emo√ß√£o (frustra√ß√£o/satisfa√ß√£o/confian√ßa)
    USER->>FEEDBACK: A√ß√£o (aceitar/editar/deletar)
    FEEDBACK->>FEEDBACK: Integra (70% impl√≠cito + 30% emocional)
    
    FEEDBACK->>REPLAY: Feedback integrado
    REPLAY->>REPLAY: Avalia import√¢ncia<br/>(Prioriza satisfa√ß√£o)
    REPLAY->>POSTGRES: Persiste conhecimento importante
    
    Note over RL,BACKPROP: Durante uso: Apenas coleta feedback<br/>Sem treinamento de modelos
    Note over RL,BACKPROP: Treinamento acontece apenas durante sono
```

**O Que Acontece**:
1. Usu√°rio interage, recebe sugest√µes
2. Feedback √© capturado (emocional + impl√≠cito)
3. Replay Buffer avalia import√¢ncia (prioriza satisfa√ß√£o)
4. Conhecimento importante √© persistido no PostgreSQL
5. **Nenhum treinamento durante uso** (apenas coleta de feedback)
6. Treinamento acontece apenas durante sono (consolida√ß√£o)

---

### Noite: Consolida√ß√£o (Sono)

```mermaid
sequenceDiagram
    participant POSTGRES as PostgreSQL<br/>(Hipocampo)
    participant REPLAY as Replay Buffer
    participant MAS as MAS
    participant FT as Fine-tuning
    participant LLM as LLM Base<br/>(PFC)
    participant LORA as LoRA Adapters
    participant CEREB as Cerebelo
    
    Note over POSTGRES,CEREB: Durante "Sono" (Consolida√ß√£o)
    
    POSTGRES->>REPLAY: Conhecimento acumulado
    REPLAY->>REPLAY: Filtra por feedback emocional<br/>(Score > 0.7, prioriza satisfa√ß√£o)
    REPLAY->>MAS: Conhecimento importante
    
    MAS->>MAS: Calcula import√¢ncia de par√¢metros
    MAS->>MAS: Preserva conhecimento antigo importante
    
    MAS->>FT: Dataset de treinamento filtrado
    FT->>FT: Fine-tuning com MAS<br/>(Preserva conhecimento antigo)
    
    Note over FT: LLM Base N√ÉO √© treinada<br/>(plug-and-play)
    FT->>LORA: Consolida adapters (essencial)
    FT->>CEREB: Consolida padr√µes importantes (essencial)
    FT->>MOD: Atualiza Modulador (se necess√°rio, opcional)
    FT->>ATT: Atualiza Aten√ß√£o (se necess√°rio, opcional)
    
    Note over LLM,CEREB: Consolida√ß√£o Completa
```

**O Que Acontece**:
1. Sistema detecta inatividade
2. PostgreSQL acumulou conhecimento suficiente
3. Replay Buffer filtra por feedback emocional (prioriza satisfa√ß√£o)
4. MAS preserva conhecimento antigo importante
5. Fine-tuning consolida conhecimento
6. **LLM Base N√ÉO √© treinada** (permanece plug-and-play)
7. LoRA Adapters s√£o consolidados (essencial)
8. Cerebelo consolida padr√µes importantes (essencial)
9. Modulador e Aten√ß√£o s√£o atualizados apenas se necess√°rio (opcional)

---

### Pr√≥ximo Dia: Conhecimento Consolidado

**O Que Mudou**:
1. **LLM Base** permanece igual (plug-and-play, n√£o treinada)
2. **LoRA Adapters** est√£o atualizados (treinados durante sono)
3. **Cerebelo** tem padr√µes importantes consolidados (treinado durante sono)
4. **Modulador** pode ter aprendido padr√µes (se treinado, opcional)
5. **Aten√ß√£o Neuromodulada** pode focar melhor (se treinada, opcional)
6. **Sistema** est√° mais inteligente (Cerebelo e LoRA melhorados)

---

## üìä Tabela: Todas as LLMs e Suas Fun√ß√µes

| LLM | Tamanho | Fun√ß√£o | Onde Usado | Treinar? | Quando? | Status |
|-----|---------|--------|------------|---------|---------|--------|
| **LLM Base** | 3B | Racioc√≠nio principal | PFC, processamento principal | ‚ùå **N√ÉO** | Nunca (plug-and-play) | ‚úÖ Implementado |
| **Modulador** | 1-5M | Sele√ß√£o de adapters | PFC, controle de adapters | ‚ö†Ô∏è **OPCIONAL** | Apenas no sono (se necess√°rio) | ‚úÖ Implementado |
| **Cerebelo** | 100M-500M | Padr√µes espec√≠ficos | Automatiza√ß√£o, padr√µes | ‚úÖ **SIM** | Apenas no sono | ‚ö†Ô∏è Experimental |
| **Aten√ß√£o Neuromodulada** | Mecanismo | Controle contextual | Aten√ß√£o do LLM Base | ‚ö†Ô∏è **OPCIONAL** | Apenas no sono (se necess√°rio) | ‚ö†Ô∏è Experimental |
| **LoRA Adapters** | Pesos adicionais | Adapta√ß√£o r√°pida | Especializa√ß√£o por contexto | ‚úÖ **SIM** | Apenas no sono | ‚úÖ Implementado |
| **Modelos Especializados** | Pequenos | Processos psicol√≥gicos | Percep√ß√£o, emo√ß√£o, etc. | ‚ö†Ô∏è **OPCIONAL** | Apenas no sono (se necess√°rio) | ‚ö†Ô∏è Planejado |

**Total**: 3-4 modelos principais + m√∫ltiplos adapters + mecanismos especializados

---

## üéØ Resumo: Como Funciona no Dia-a-Dia

### Ciclo Completo

```
Manh√£: Intera√ß√£o
  ‚Üì
Durante o Dia: Aprendizado Cont√≠nuo
  ‚Üì
Noite: Consolida√ß√£o (Sono)
  ‚Üì
Pr√≥ximo Dia: Conhecimento Consolidado
  ‚Üì
Repete...
```

### Componentes em A√ß√£o

1. **LLM Base**: Processamento principal, racioc√≠nio
2. **Modulador**: Sele√ß√£o inteligente de adapters
3. **Cerebelo**: Padr√µes espec√≠ficos, automatiza√ß√£o
4. **Aten√ß√£o Neuromodulada**: Foco no relevante
5. **LoRA Adapters**: Adapta√ß√£o r√°pida por contexto
6. **Backpropamine**: Aprendizado real (experimental)
7. **MAS**: Preserva√ß√£o de conhecimento
8. **Replay**: Prioriza√ß√£o de satisfa√ß√£o
9. **Feedback Emocional**: Guia aprendizado

### Fluxo de Aprendizado

```
Intera√ß√£o ‚Üí Feedback ‚Üí Coleta ‚Üí Consolida√ß√£o (Sono) ‚Üí Melhoria
    ‚Üì           ‚Üì          ‚Üì            ‚Üì                  ‚Üì
  Query    Emo√ß√£o +    PostgreSQL    Cerebelo +      Sistema Mais
  C√≥digo   A√ß√£o        (sem treino)  LoRA treinam    Inteligente
                                      (LLM Base n√£o)
```

---

## üìä Classifica√ß√£o: O Que Treinar e O Que N√£o Treinar

### Resumo da Classifica√ß√£o

| LLM | Treinar? | Quando? | Justificativa |
|-----|----------|---------|---------------|
| **LLM Base** | ‚ùå **N√ÉO** | Nunca | Plug-and-play, pode ser trocada |
| **Cerebelo** | ‚úÖ **SIM** | Apenas no sono | Essencial para padr√µes espec√≠ficos |
| **Modulador** | ‚ö†Ô∏è **OPCIONAL** | Apenas no sono (se necess√°rio) | Pode funcionar apenas com infer√™ncia |
| **Aten√ß√£o Neuromodulada** | ‚ö†Ô∏è **OPCIONAL** | Apenas no sono (se necess√°rio) | Pode usar aten√ß√£o padr√£o do LLM |
| **LoRA Adapters** | ‚úÖ **SIM** | Apenas no sono | Essencial para adapta√ß√£o por contexto |

---

## üî¨ Detalhamento: Como Cada LLM Aprende

### LLM Base (CodeLlama 3B)

**Aprendizado**:
- ‚ùå **N√ÉO TREINAR**: Plug-and-play, n√£o modificar
- ‚úÖ **Usar como est√°**: Modelo pr√©-treinado
- ‚úÖ **Pode ser trocada**: Por qualquer LLM compat√≠vel

**Justificativa**:
- √â componente base, n√£o deve ser modificado
- Permite trocar por modelos melhores sem perder conhecimento
- Conhecimento fica no PostgreSQL (Hipocampo) e LoRA Adapters

**Quando Usa**:
- Durante uso: Infer√™ncia apenas
- Conhecimento vem do RAG (PostgreSQL) e LoRA Adapters

---

### Modulador (1-5M)

**Aprendizado**:
- ‚ö†Ô∏è **OPCIONAL**: Pode funcionar apenas com infer√™ncia
- ‚ö†Ô∏è **Se treinar**: Apenas durante sono (consolida√ß√£o)
- ‚ùå **N√£o treinar durante uso**: Evita overhead

**Justificativa**:
- Pode funcionar apenas com infer√™ncia baseada em contexto
- Se necess√°rio aprender, apenas no sono para evitar overhead
- Treinamento durante uso pode ser muito custoso

**Quando Aprende** (se necess√°rio):
- Durante consolida√ß√£o (sono)
- Baseado em feedback emocional + impl√≠cito
- Fine-tuning com RL (PPO) apenas no sono

---

### Cerebelo (100M-500M)

**Aprendizado**:
- ‚úÖ **ESSENCIAL TREINAR**: Mas apenas durante sono
- ‚ùå **N√£o treinar durante uso**: Evita overhead
- ‚úÖ **Durante sono**: Backpropamine + consolida√ß√£o de padr√µes
- ‚úÖ **Preserva√ß√£o**: MAS protege padr√µes importantes

**Justificativa**:
- √â essencial para padr√µes espec√≠ficos e automatiza√ß√£o
- Treinar apenas no sono evita overhead durante uso
- Backpropamine pode ser usado, mas apenas durante consolida√ß√£o

**Quando Aprende**:
- Durante consolida√ß√£o (sono) apenas
- Padr√µes que geram satisfa√ß√£o (priorizados)
- Backpropamine aplicado apenas durante sono

---

### Aten√ß√£o Neuromodulada

**Aprendizado**:
- ‚ö†Ô∏è **OPCIONAL**: Pode usar aten√ß√£o padr√£o do LLM
- ‚ö†Ô∏è **Se treinar**: Apenas durante sono (consolida√ß√£o)
- ‚ùå **N√£o treinar durante uso**: Evita overhead

**Justificativa**:
- Aten√ß√£o padr√£o do LLM pode ser suficiente
- Se necess√°rio neuromodula√ß√£o, apenas no sono
- Treinamento durante uso pode ser muito custoso

**Quando Aprende** (se necess√°rio):
- Durante consolida√ß√£o (sono) apenas
- Baseado em feedback emocional (focar no que gera satisfa√ß√£o)
- Backpropamine aplicado apenas durante sono

---

### LoRA Adapters

**Aprendizado**:
- ‚úÖ **ESSENCIAL TREINAR**: Mas apenas durante sono
- ‚ùå **N√£o treinar durante uso**: Evita overhead
- ‚úÖ **Durante sono**: Consolida√ß√£o e fine-tuning
- ‚úÖ **Preserva√ß√£o**: MAS protege conhecimento importante

**Justificativa**:
- √â essencial para adapta√ß√£o por contexto
- Treinar apenas no sono evita overhead durante uso
- Conhecimento importante √© consolidado durante sono

**Quando Aprende**:
- Durante consolida√ß√£o (sono) apenas
- Baseado em feedback emocional + impl√≠cito
- Fine-tuning incremental apenas durante sono

---

## üéØ Conclus√£o: Funcionamento do Dia-a-Dia

### Arquitetura Completa com Todas as LLMs

O sistema npllm utiliza:

1. **3-4 modelos principais**:
   - LLM Base (3B) - Racioc√≠nio principal
   - Modulador (1-5M) - Sele√ß√£o de adapters
   - Cerebelo (100M-500M) - Padr√µes espec√≠ficos (experimental)
   - Aten√ß√£o Neuromodulada - Controle contextual (experimental)

2. **M√∫ltiplos LoRA Adapters**:
   - Adapta√ß√£o r√°pida por contexto
   - Especializa√ß√£o por projeto

3. **Sistema de aprendizado integrado**:
   - MAS (preserva√ß√£o)
   - Replay (mem√≥rias importantes, prioriza satisfa√ß√£o)
   - Backpropamine (plasticidade real - experimental)
   - RL PPO (sistema dopamin√©rgico)

4. **Sistema de feedback emocional**:
   - Feedback impl√≠cito (70%)
   - Feedback emocional (30%)
   - Integra√ß√£o para prioriza√ß√£o

5. **Sistema de consolida√ß√£o**:
   - Durante sono
   - Filtragem por feedback emocional
   - Transfer√™ncia apenas para Cerebelo e LoRA Adapters (n√£o para LLM Base)

### Funcionamento Di√°rio

- **Manh√£**: Intera√ß√µes, processamento com todas as LLMs (infer√™ncia apenas)
- **Durante o Dia**: Feedback coletado, conhecimento armazenado no PostgreSQL (sem treinamento)
- **Noite**: Consolida√ß√£o (sono), apenas Cerebelo e LoRA Adapters s√£o treinados
- **Pr√≥ximo Dia**: Sistema mais inteligente, Cerebelo e LoRA Adapters melhorados (LLM Base permanece igual)

---

**Data de Cria√ß√£o**: 2025-01-27  
**√öltima Atualiza√ß√£o**: 2025-01-27  
**Status**: ‚úÖ Completo - Arquitetura Completa Revisada com Todas as LLMs

