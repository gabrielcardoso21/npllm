# Arquitetura de Aprendizado Real e Funcionamento do Dia-a-Dia

**Data**: 2025-01-27  
**Vers√£o**: 1.0  
**Status**: üìä Foco: Aprendizado Real + Funcionamento Di√°rio

---

## üìã Sum√°rio Executivo

Este documento foca **apenas no aprendizado real e funcionamento do dia-a-dia**, sem integra√ß√£o com Linux. Inclui:

1. **Aprendizado Real**: Backpropamine, MAS, Replay
2. **Funcionamento Di√°rio**: Intera√ß√µes, feedback, consolida√ß√£o
3. **Sistema de Mem√≥ria**: Curto, m√©dio e longo prazo
4. **Sistema de Feedback**: Emocional + impl√≠cito
5. **Consolida√ß√£o**: Durante sono

**Foco**: C√©rebro (SNC) apenas, sem corpo (SNP/Linux).

---

## üß† Arquitetura Simplificada: Apenas C√©rebro

```mermaid
graph TB
    subgraph "Entrada"
        USER[Usu√°rio<br/>Query/C√≥digo/Feedback]
    end
    
    subgraph "Mem√≥ria de Curto Prazo"
        CACHE[Cache R√°pido<br/>Redis/Mem√≥ria<br/>‚ö° Muito R√°pido]
        WORKING[Mem√≥ria de Trabalho<br/>Contexto Atual]
    end
    
    subgraph "Mem√≥ria de M√©dio Prazo"
        POSTGRES[PostgreSQL + pgvector<br/>Hipocampo<br/>üíæ Persistente]
    end
    
    subgraph "LLMs e Processamento"
        LLM_BASE[LLM Base<br/>CodeLlama 3B<br/>PFC - Racioc√≠nio]
        MODULATOR[Modulador<br/>1-5M par√¢metros<br/>Sele√ß√£o Adapters]
        LORA[LoRA Adapters<br/>Adapta√ß√£o R√°pida]
    end
    
    subgraph "Aprendizado Real"
        MAS[MAS<br/>Preserva√ß√£o<br/>Conhecimento Importante]
        REPLAY[Replay Buffer<br/>Mem√≥rias Importantes<br/>Prioriza Satisfa√ß√£o]
        BACKPROP[Backpropamine*<br/>Plasticidade Real<br/>Modulador + Aten√ß√£o]
    end
    
    subgraph "Sistema de Feedback"
        IMPLICIT[Feedback Impl√≠cito<br/>Aceitar/Editar/Deletar<br/>70%]
        EMOTIONAL[Feedback Emocional<br/>Frustra√ß√£o/Satisfa√ß√£o/Confian√ßa<br/>30%]
        INTEGRATE[Integra√ß√£o Feedback<br/>r = 0.7 * impl√≠cito + 0.3 * emocional]
    end
    
    subgraph "Consolida√ß√£o (Sono)"
        SLEEP[Consolida√ß√£o Durante Sono<br/>Hipocampo ‚Üí C√≥rtex]
        FT[Fine-tuning<br/>Incremental]
    end
    
    subgraph "Sa√≠da"
        RESPONSE[Resposta/Sugest√£o<br/>C√≥digo Arquitetural]
    end
    
    USER --> CACHE
    CACHE -->|Consulta| POSTGRES
    POSTGRES -->|Resultados| CACHE
    CACHE --> WORKING
    WORKING --> POSTGRES
    POSTGRES --> LLM_BASE
    
    LLM_BASE --> MODULATOR
    MODULATOR --> LORA
    LORA --> LLM_BASE
    LLM_BASE --> RESPONSE
    
    USER --> IMPLICIT
    USER --> EMOTIONAL
    IMPLICIT --> INTEGRATE
    EMOTIONAL --> INTEGRATE
    INTEGRATE --> REPLAY
    INTEGRATE --> BACKPROP
    
    REPLAY --> POSTGRES
    
    Note over REPLAY,BACKPROP: Durante uso: Apenas coleta feedback<br/>Sem treinamento
    
    POSTGRES --> SLEEP
    SLEEP --> FT
    FT --> CEREBELO
    FT --> LORA
    
    Note over FT: LLM Base N√ÉO √© treinada<br/>(plug-and-play)
    
    MAS --> CEREBELO
    MAS --> LORA
    
    RESPONSE --> USER
    
    style LLM_BASE fill:#e1f5ff
    style MODULATOR fill:#fff4e1
    style LORA fill:#ffe1f5
    style POSTGRES fill:#ccffcc
    style MAS fill:#ffcccc
    style REPLAY fill:#ffffcc
    style BACKPROP fill:#ffcccc
    style INTEGRATE fill:#ffccff
    style SLEEP fill:#ccccff
```

**Legenda**:
- `*` = Componente experimental (Backpropamine)
- Cores diferentes = Diferentes subsistemas

---

## üîÑ Fluxo do Dia-a-Dia: Intera√ß√£o ‚Üí Aprendizado ‚Üí Consolida√ß√£o

```mermaid
sequenceDiagram
    participant USER as Usu√°rio
    participant CACHE as Cache R√°pido
    participant POSTGRES as PostgreSQL<br/>(Hipocampo)
    participant LLM as LLM Base<br/>(PFC)
    participant MOD as Modulador
    participant LORA as LoRA Adapters
    participant REPLAY as Replay Buffer
    participant FEEDBACK as Feedback<br/>(Emocional + Impl√≠cito)
    participant MAS as MAS
    participant BACKPROP as Backpropamine*
    participant SLEEP as Consolida√ß√£o<br/>(Sono)
    
    Note over USER,SLEEP: Dia-a-Dia: Intera√ß√£o e Aprendizado
    
    USER->>CACHE: Query/C√≥digo
    CACHE->>POSTGRES: Busca sem√¢ntica (se n√£o encontrar)
    POSTGRES-->>CACHE: Resultados relevantes
    CACHE->>LLM: Contexto + Query
    
    LLM->>MOD: Hidden states
    MOD->>MOD: Analisa contexto
    MOD->>LORA: Seleciona adapters
    LORA->>LLM: Adapta√ß√£o aplicada
    LLM->>USER: Resposta/Sugest√£o
    
    USER->>FEEDBACK: Emo√ß√£o (frustra√ß√£o/satisfa√ß√£o/confian√ßa)
    USER->>FEEDBACK: A√ß√£o (aceitar/editar/deletar)
    FEEDBACK->>FEEDBACK: Integra (70% impl√≠cito + 30% emocional)
    FEEDBACK->>REPLAY: Feedback integrado
    FEEDBACK->>BACKPROP: Sinal de recompensa
    
    REPLAY->>REPLAY: Avalia import√¢ncia<br/>(Prioriza satisfa√ß√£o)
    REPLAY->>POSTGRES: Persiste conhecimento importante
    REPLAY->>MAS: Marca para preserva√ß√£o
    
    BACKPROP->>MOD: Atualiza pesos (plasticidade real)
    BACKPROP->>MAS: Preserva conhecimento antigo
    
    Note over POSTGRES,SLEEP: Durante "Sono" (Consolida√ß√£o)
    POSTGRES->>REPLAY: Conhecimento acumulado
    REPLAY->>REPLAY: Filtra por feedback emocional<br/>(Score > 0.7)
    REPLAY->>MAS: Conhecimento importante
    MAS->>MAS: Preserva conhecimento antigo
    MAS->>SLEEP: Dataset de treinamento
    SLEEP->>LLM: Atualiza pesos (fine-tuning)
    SLEEP->>LORA: Consolida adapters
```

---

## üß† Componentes de Aprendizado Real

### 1. MAS (Memory Aware Synapses)

**Fun√ß√£o**: Preserva conhecimento importante durante aprendizado

**Como Funciona**:
1. Calcula import√¢ncia de cada par√¢metro baseado em gradientes
2. Durante novo aprendizado, penaliza mudan√ßas em par√¢metros importantes
3. Preserva conhecimento antigo enquanto aprende novo

**F√≥rmula**:
```
Loss_total = Loss_novo + Œª * Œ£(import√¢ncia * (peso_atual - peso_antigo)¬≤)
```

**Onde √© Usado**:
- Durante consolida√ß√£o (sono)
- Durante fine-tuning incremental
- Preserva conhecimento consolidado na LLM Base

**Status**: ‚úÖ Implementado (mas n√£o testado)

---

### 2. Replay Buffer

**Fun√ß√£o**: Reapresenta mem√≥rias importantes durante aprendizado

**Como Funciona**:
1. Armazena experi√™ncias com feedback positivo (satisfa√ß√£o/confian√ßa)
2. Durante treinamento, mistura exemplos antigos com novos
3. Prioriza padr√µes que geram satisfa√ß√£o

**Crit√©rio de Sele√ß√£o**:
- Score > 0.7: Mant√©m (importante)
- Score < -0.3: Marca como evitar (frustra√ß√£o)
- Entre -0.3 e 0.7: Descarta (n√£o relevante)

**Onde √© Usado**:
- Durante treinamento de adapters
- Durante consolida√ß√£o (sono)
- Prioriza conhecimento que gera satisfa√ß√£o

**Status**: ‚úÖ Implementado (mas n√£o testado)

---

### 3. Backpropamine (Experimental)

**Fun√ß√£o**: Mudan√ßas reais de pesos baseadas em atividade

**Como Funciona**:
1. Cada conex√£o tem: peso base + peso pl√°stico
2. Peso pl√°stico adapta-se baseado em atividade
3. Trein√°vel via backpropagation
4. Permite aprendizado cont√≠nuo sem esquecer

**F√≥rmula**:
```
Peso_Total = Peso_Base + (Peso_Pl√°stico √ó Atividade)
```

**Onde √© Usado** (Experimental):
- **Modulador**: Aprendizado r√°pido de sele√ß√£o de adapters
- **Aten√ß√£o**: Neuromodula√ß√£o contextual
- **Cerebelo**: Padr√µes espec√≠ficos (futuro)

**Status**: ‚ö†Ô∏è Experimental (Fase 2)

---

## üìä Sistema de Feedback Integrado

### Feedback Impl√≠cito (70%)

**Fonte**: A√ß√µes do usu√°rio

**Sinais**:
- **Aceitar c√≥digo**: +1.0 (recompensa m√°xima)
- **Editar c√≥digo**: +0.3 a +0.8 (depende da dist√¢ncia de edi√ß√£o)
- **Deletar c√≥digo**: -0.5 (recompensa negativa)
- **Ignorar**: -0.1 (recompensa negativa leve)

**Caracter√≠sticas**:
- ‚úÖ Objetivo, baseado em a√ß√£o real
- ‚úÖ Sinal forte e confi√°vel
- ‚úÖ Dispon√≠vel imediatamente

---

### Feedback Emocional (30%)

**Fonte**: Emo√ß√µes do usu√°rio

**Sinais**:
- **Satisfa√ß√£o**: +0.8 a +1.0 (intensidade)
- **Confian√ßa**: +0.9 a +1.0 (intensidade)
- **Frustra√ß√£o**: -1.0 a 0.0 (intensidade)
- **Neutro**: 0.0

**Caracter√≠sticas**:
- ‚ö†Ô∏è Subjetivo, mas importante para intera√ß√£o humana
- ‚ö†Ô∏è Sinal mais fraco que impl√≠cito
- ‚ö†Ô∏è Pode ser ruidoso

**Detec√ß√£o**:
- An√°lise de sentimento (RoBERTa)
- Detec√ß√£o de emo√ß√µes espec√≠ficas (futuro)
- Coment√°rios do usu√°rio
- Texto de feedback

---

### Integra√ß√£o de Feedback

**F√≥rmula**:
```
r_total = 0.7 * r_impl√≠cito + 0.3 * r_emocional
```

**Onde**:
- `r_impl√≠cito` = -1.0 a +1.0
- `r_emocional` = -1.0 a +1.0
- `r_total` = -1.0 a +1.0

**Uso**:
- **Replay Buffer**: Prioriza conhecimento com r_total > 0.7
- **RL PPO**: Recompensa para treinar Modulador
- **Backpropamine**: Sinal de recompensa para plasticidade
- **Consolida√ß√£o**: Filtra conhecimento durante sono

---

## üí§ Consolida√ß√£o Durante "Sono"

### Processo Completo

```mermaid
graph TB
    subgraph "Fase 1: Coleta"
        POSTGRES[PostgreSQL<br/>Conhecimento Acumulado]
        REPLAY[Replay Buffer<br/>Experi√™ncias Importantes]
    end
    
    subgraph "Fase 2: Filtragem"
        FILTER[Filtro por Feedback<br/>‚úÖ Satisfa√ß√£o/Confian√ßa<br/>‚ùå Frustra√ß√£o]
        PRIORITY[Prioriza√ß√£o<br/>Padr√µes que Geram Satisfa√ß√£o]
    end
    
    subgraph "Fase 3: Preserva√ß√£o"
        MAS[MAS<br/>Preserva Conhecimento Antigo<br/>Importante]
        IMPORTANCE[Calcula Import√¢ncia<br/>de Par√¢metros]
    end
    
    subgraph "Fase 4: Treinamento"
        DATASET[Dataset de Treinamento<br/>Conhecimento Filtrado]
        FT[Fine-tuning<br/>com MAS<br/>Preserva Antigo]
    end
    
    subgraph "Fase 5: Consolida√ß√£o"
        CEREBELO[Cerebelo<br/>Consolidado<br/>Essencial]
        LORA[LoRA Adapters<br/>Consolidados<br/>Essencial]
        MOD[Modulador<br/>Opcional]
        ATT[Aten√ß√£o<br/>Opcional]
    end
    
    POSTGRES --> FILTER
    REPLAY --> FILTER
    FILTER --> PRIORITY
    PRIORITY --> MAS
    MAS --> IMPORTANCE
    IMPORTANCE --> DATASET
    DATASET --> FT
    FT --> CEREBELO
    FT --> LORA
    FT --> MOD
    FT --> ATT
    
    Note over FT: LLM Base N√ÉO √© treinada<br/>(plug-and-play)
    
    style POSTGRES fill:#ccffcc
    style REPLAY fill:#ffffcc
    style FILTER fill:#ffcccc
    style PRIORITY fill:#ffcccc
    style MAS fill:#ccccff
    style IMPORTANCE fill:#ccccff
    style DATASET fill:#fff4e1
    style FT fill:#fff4e1
    style LLM_BASE fill:#e1f5ff
    style LORA fill:#ffe1f5
```

### Quando Acontece

1. **Per√≠odo de Inatividade**: Sistema detecta que usu√°rio n√£o est√° usando
2. **Ac√∫mulo de Conhecimento**: PostgreSQL tem conhecimento suficiente para consolidar
3. **Agendamento**: Peri√≥dico (ex: di√°rio, semanal)

### O Que Acontece

1. **Coleta**: Extrai conhecimento do PostgreSQL
2. **Filtragem**: Filtra por feedback emocional (prioriza satisfa√ß√£o)
3. **Preserva√ß√£o**: MAS preserva conhecimento antigo importante
4. **Treinamento**: Fine-tuning incremental com MAS
5. **Consolida√ß√£o**: 
   - ‚ùå **LLM Base N√ÉO √© treinada** (plug-and-play)
   - ‚úÖ **Cerebelo** √© consolidado (essencial)
   - ‚úÖ **LoRA Adapters** s√£o consolidados (essencial)
   - ‚ö†Ô∏è **Modulador** e **Aten√ß√£o** s√£o atualizados apenas se necess√°rio (opcional)

---

## üîÑ Ciclo Completo: Dia-a-Dia

### Manh√£: Primeiras Intera√ß√µes

1. **Usu√°rio** faz query sobre arquitetura
2. **Cache** verifica se tem resposta
3. Se n√£o, **PostgreSQL** busca sem√¢ntica
4. **LLM Base** processa com contexto
5. **Modulador** seleciona adapters apropriados
6. **Resposta** √© gerada e apresentada

### Durante o Dia: Coleta de Feedback (Sem Treinamento)

1. **Usu√°rio** interage, recebe sugest√µes
2. **Feedback** √© capturado (emocional + impl√≠cito)
3. **Replay Buffer** avalia import√¢ncia
4. **Conhecimento importante** √© persistido no PostgreSQL
5. **Nenhum treinamento durante uso** (apenas coleta de feedback)
6. Treinamento acontece apenas durante sono

### Noite: Consolida√ß√£o (Sono)

1. **Sistema** detecta inatividade
2. **PostgreSQL** acumulou conhecimento suficiente
3. **Replay Buffer** filtra por feedback emocional
4. **MAS** preserva conhecimento antigo importante
5. **Fine-tuning** consolida conhecimento
6. **LLM Base N√ÉO √© treinada** (permanece plug-and-play)
7. **Cerebelo** √© consolidado (essencial)
8. **LoRA Adapters** s√£o consolidados (essencial)
9. **Modulador** e **Aten√ß√£o** s√£o atualizados apenas se necess√°rio (opcional)

### Pr√≥ximo Dia: Conhecimento Consolidado

1. **LLM Base** permanece igual (plug-and-play, n√£o treinada)
2. **Cerebelo** tem padr√µes importantes consolidados (treinado durante sono)
3. **LoRA Adapters** est√£o atualizados (treinados durante sono)
4. **Modulador** pode ter aprendido padr√µes (se treinado, opcional)
5. **Sistema** est√° mais inteligente (Cerebelo e LoRA melhorados)

---

## üìä Tabela: Componentes de Aprendizado Real

| Componente | Fun√ß√£o | Status | Prioridade |
|------------|--------|--------|------------|
| **MAS** | Preserva conhecimento importante | ‚úÖ Implementado | üü° Alta |
| **Replay Buffer** | Reapresenta mem√≥rias importantes | ‚úÖ Implementado | üü° Alta |
| **Backpropamine** | Plasticidade real (mudan√ßas de pesos) | ‚ö†Ô∏è Experimental | üîµ Baixa |
| **Feedback Emocional** | Prioriza conhecimento satisfat√≥rio | ‚ö†Ô∏è B√°sico | üî¥ Cr√≠tica |
| **Feedback Impl√≠cito** | Recompensa baseada em a√ß√µes | ‚úÖ Implementado | üî¥ Cr√≠tica |
| **Consolida√ß√£o Sono** | Transfer√™ncia hipocampo ‚Üí Cerebelo/LoRA | ‚ö†Ô∏è Planejado | üü° Alta |

---

## üìä Classifica√ß√£o: O Que Treinar e O Que N√£o Treinar

### Resumo da Classifica√ß√£o

| LLM | Treinar? | Quando? | Justificativa |
|-----|----------|---------|---------------|
| **LLM Base** | ‚ùå **N√ÉO** | Nunca | Plug-and-play, pode ser trocada |
| **Cerebelo** | ‚úÖ **SIM** | Apenas no sono | Essencial para padr√µes espec√≠ficos |
| **Modulador** | ‚ö†Ô∏è **OPCIONAL** | Apenas no sono (se necess√°rio) | Pode funcionar apenas com infer√™ncia |
| **Aten√ß√£o Neuromodulada** | ‚ö†Ô∏è **OPCIONAL** | Apenas no sono (se necess√°rio) | Pode usar aten√ß√£o padr√£o do LLM |
| **LoRA Adapters** | ‚úÖ **SIM** | Apenas no sono | Essencial para adapta√ß√£o por contexto |

---

## üéØ Resumo: Aprendizado Real e Funcionamento Di√°rio

### Aprendizado Real

1. **MAS**: Preserva conhecimento importante durante aprendizado
2. **Replay**: Reapresenta mem√≥rias importantes, prioriza satisfa√ß√£o
3. **Backpropamine**: Mudan√ßas reais de pesos (experimental, no Modulador)

### Funcionamento Di√°rio

1. **Intera√ß√£o**: Usu√°rio ‚Üí Cache ‚Üí PostgreSQL ‚Üí LLM ‚Üí Resposta (infer√™ncia apenas)
2. **Feedback**: Emocional (30%) + Impl√≠cito (70%) ‚Üí Replay ‚Üí PostgreSQL (sem treinamento)
3. **Aprendizado**: Apenas coleta de feedback durante uso (sem treinamento)
4. **Consolida√ß√£o**: Durante sono, conhecimento vai para Cerebelo e LoRA Adapters (n√£o para LLM Base)

### Fluxo Cont√≠nuo

```
Intera√ß√£o ‚Üí Feedback ‚Üí Coleta ‚Üí Consolida√ß√£o (Sono) ‚Üí Melhoria
    ‚Üì           ‚Üì          ‚Üì            ‚Üì                  ‚Üì
  Query    Emo√ß√£o +    PostgreSQL    Cerebelo +      Sistema Mais
  C√≥digo   A√ß√£o        (sem treino)  LoRA treinam    Inteligente
                                      (LLM Base n√£o)
```

---

## üî¨ Detalhamento T√©cnico

### MAS: Como Preserva Conhecimento

```python
def compute_importance(model, dataloader):
    """
    Calcula import√¢ncia de par√¢metros baseado em gradientes
    """
    importance = {}
    for name, param in model.named_parameters():
        if param.requires_grad:
            importance[name] = torch.zeros_like(param)
    
    for batch in dataloader:
        loss = compute_loss(model, batch)
        loss.backward()
        
        # Acumula magnitude dos gradientes
        for name, param in model.named_parameters():
            if param.requires_grad and param.grad is not None:
                importance[name] += param.grad.abs()
    
    return importance

def mas_regularization_loss(model, importance, old_params, lambda_reg=0.5):
    """
    Calcula loss de regulariza√ß√£o MAS
    """
    reg_loss = 0.0
    for name, param in model.named_parameters():
        if name in importance and name in old_params:
            diff = param - old_params[name]
            reg_loss += (importance[name] * diff.pow(2)).sum()
    
    return lambda_reg * reg_loss
```

### Replay: Como Prioriza Satisfa√ß√£o

```python
def decide_persistence(experience, emotional_feedback, implicit_feedback):
    """
    Decide se experi√™ncia deve ser persistida baseado em feedback
    """
    # Score emocional
    emotional_score = 0.0
    if emotional_feedback.sentiment == "satisfaction":
        emotional_score = 0.8 + (emotional_feedback.intensity * 0.2)
    elif emotional_feedback.sentiment == "confidence":
        emotional_score = 0.9 + (emotional_feedback.intensity * 0.1)
    elif emotional_feedback.sentiment == "frustration":
        emotional_score = 0.0 - (emotional_feedback.intensity * 0.5)
    
    # Score impl√≠cito
    implicit_score = 0.0
    if implicit_feedback.action == "accept":
        implicit_score = 1.0
    elif implicit_feedback.action == "edit":
        implicit_score = 0.5 - (implicit_feedback.edit_distance * 0.3)
    elif implicit_feedback.action == "delete":
        implicit_score = -0.5
    
    # Score combinado (70% impl√≠cito + 30% emocional)
    combined_score = (0.7 * implicit_score) + (0.3 * emotional_score)
    
    # Decis√£o
    if combined_score > 0.7:
        return "keep", combined_score
    elif combined_score < -0.3:
        return "avoid", combined_score
    else:
        return "discard", combined_score
```

### Backpropamine: Como Funciona (Experimental)

```python
class PlasticLayer(nn.Module):
    """
    Camada com plasticidade diferenci√°vel
    """
    def __init__(self, in_features, out_features):
        super().__init__()
        # Peso base (est√°tico)
        self.base_weight = nn.Parameter(torch.randn(out_features, in_features))
        # Peso pl√°stico (adapt√°vel)
        self.plastic_weight = nn.Parameter(torch.randn(out_features, in_features))
        # Atividade (acumulada)
        self.activity = torch.zeros(out_features, in_features)
    
    def forward(self, x):
        # Atualiza atividade baseada em entrada
        self.activity = 0.9 * self.activity + 0.1 * (x.unsqueeze(0) * x.unsqueeze(1))
        
        # Peso total = base + pl√°stico * atividade
        weight = self.base_weight + self.plastic_weight * self.activity
        
        return F.linear(x, weight)
```

---

## üéØ Conclus√£o

### Foco: Aprendizado Real e Funcionamento Di√°rio

Este documento foca **apenas no c√©rebro** (aprendizado real e funcionamento di√°rio), sem integra√ß√£o com Linux:

1. **Aprendizado Real**:
   - MAS preserva conhecimento importante
   - Replay prioriza satisfa√ß√£o
   - Backpropamine permite mudan√ßas reais (experimental)

2. **Funcionamento Di√°rio**:
   - Intera√ß√£o ‚Üí Feedback ‚Üí Aprendizado ‚Üí Consolida√ß√£o
   - Ciclo cont√≠nuo de melhoria
   - Sistema fica mais inteligente com o tempo

3. **Sem Integra√ß√£o Linux**:
   - Foco apenas no c√©rebro (SNC)
   - Sem sistema sensorial/motor
   - Apenas aprendizado e consolida√ß√£o

---

**Data de Cria√ß√£o**: 2025-01-27  
**√öltima Atualiza√ß√£o**: 2025-01-27  
**Status**: ‚úÖ Completo - Foco: Aprendizado Real + Funcionamento Di√°rio

